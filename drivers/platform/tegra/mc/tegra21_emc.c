/*
 * drivers/platform/tegra/tegra21_emc.c
 *
 * Copyright (c) 2014, NVIDIA CORPORATION.  All rights reserved.
 *
 * This program is free software; you can redistribute it and/or modify
 * it under the terms of the GNU General Public License version 2 as
 * published by the Free Software Foundation.
 *
 * This program is distributed in the hope that it will be useful, but WITHOUT
 * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
 * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
 * more details.
 */

#include <linux/kernel.h>
#include <linux/clk.h>
#include <linux/err.h>
#include <linux/io.h>
#include <linux/of.h>
#include <linux/module.h>
#include <linux/delay.h>
#include <linux/debugfs.h>
#include <linux/seq_file.h>
#include <linux/hrtimer.h>
#include <linux/pasr.h>
#include <linux/slab.h>
#include <linux/platform_device.h>
#include <linux/tegra-soc.h>
#include <linux/platform_data/tegra_emc_pdata.h>

#include <asm/cputime.h>

#include <tegra/tegra21_emc.h>
#include <tegra/mc-regs-t21x.h>

#include <mach/nct.h>

#include <linux/platform/tegra/clock.h>
#include "board.h"
#include <linux/platform/tegra/dvfs.h>
#include "iomap.h"
#include "tegra_emc_dt_parse.h"
#include "devices.h"
#include <linux/platform/tegra/common.h>
#include "../nvdumper/nvdumper-footprint.h"

#define DVFS_CLOCK_CHANGE_VERSION	21012
#define EMC_PRELOCK_VERSION		2101

/*
 * Enable flags for more verbosity.
 */
#define INFO		(1 << 0)
#define STEPS		(1 << 1)
#define SUB_STEPS	(1 << 2)
#define PRELOCK		(1 << 3)
#define PRELOCK_STEPS	(1 << 4)
#define ACTIVE_EN	(1 << 5)
#define PRAMP_UP	(1 << 6)
#define PRAMP_DN	(1 << 7)
#define EMC_REGISTERS	(1 << 28)
#define CCFIFO		(1 << 29)
#define REGS		(1 << 30)
#define REG_LISTS	(1 << 31)

#define emc_cc_dbg(mask, fmt, ...)					\
	do {								\
		if (mask & emc_dbg_mask)				\
			pr_info("%s: " fmt, __func__, ##__VA_ARGS__);	\
	} while (0)

#if 0
static unsigned int emc_dbg_mask = INFO | STEPS | SUB_STEPS | PRELOCK |
	PRELOCK_STEPS | ACTIVE_EN | PRAMP_UP | PRAMP_DN | EMC_REGISTERS |
	CCFIFO | REGS;
#else
static unsigned int emc_dbg_mask;
#endif

#ifdef CONFIG_TEGRA_EMC_SCALING_ENABLE
static bool emc_enable = true;
#else
static bool emc_enable;
#endif
module_param(emc_enable, bool, 0644);

/* TODO: cleanup to not use iomap.h */
static void __iomem *emc_base = IO_ADDRESS(TEGRA_EMC_BASE);
static void __iomem *emc1_base = IO_ADDRESS(TEGRA_EMC1_BASE); /* Second chan. */
static void __iomem *mc_base = IO_ADDRESS(TEGRA_MC_BASE);
static void __iomem *clk_base = IO_ADDRESS(TEGRA_CLK_RESET_BASE);

#ifdef CONFIG_PASR
static int pasr_enable;
#endif

u8 tegra_emc_bw_efficiency = 80;

static u32 iso_bw_table[] = {
	5, 10, 20, 30, 40, 60, 80, 100, 120, 140, 160, 180,
	200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700
};

/*
 * These tables list the ISO efficiency (in percent) at the corresponding entry
 * in the iso_bw_table. iso_bw_table is in MHz.
 */
static u32 tegra21_lpddr3_iso_efficiency_os_idle[] = {
	64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64,
	64, 63, 60, 54, 45, 45, 45, 45, 45, 45, 45
};
static u32 tegra21_lpddr3_iso_efficiency_general[] = {
	60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
	60, 59, 59, 58, 57, 56, 55, 54, 54, 54, 54
};

static u32 tegra21_lpddr4_iso_efficiency_os_idle[] = {
	56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56, 56,
	56, 56, 56, 56, 56, 56, 56, 56, 56, 49, 45
};
static u32 tegra21_lpddr4_iso_efficiency_general[] = {
	56, 55, 55, 54, 54, 53, 51, 50, 49, 48, 47, 46,
	45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45
};

static u32 tegra21_ddr3_iso_efficiency_os_idle[] = {
	65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65,
	65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65
};
static u32 tegra21_ddr3_iso_efficiency_general[] = {
	60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,
	60, 59, 59, 58, 57, 56, 55, 54, 54, 54, 54
};

static u8 get_iso_bw_os_idle(unsigned long iso_bw);
static u8 get_iso_bw_general(unsigned long iso_bw);

static struct emc_iso_usage tegra21_emc_iso_usage[] = {
	{
		BIT(EMC_USER_DC1),
		80, get_iso_bw_os_idle
	},
	{
		BIT(EMC_USER_DC1) | BIT(EMC_USER_DC2),
		50, get_iso_bw_general
	},
	{
		BIT(EMC_USER_DC1) | BIT(EMC_USER_VI),
		50, get_iso_bw_general
	},
	{
		BIT(EMC_USER_DC1) | BIT(EMC_USER_DC2) | BIT(EMC_USER_VI),
		50, get_iso_bw_general
	},
};

#define MHZ 1000000
#define TEGRA_EMC_ISO_USE_FREQ_MAX_NUM	12
#define PLL_C_DIRECT_FLOOR		333500000
#define EMC_STATUS_UPDATE_TIMEOUT	1000
#define TEGRA_EMC_TABLE_MAX_SIZE	16

#define TEGRA_EMC_MODE_REG_17	0x00110000
#define TEGRA_EMC_MRW_DEV_SHIFT	30
#define TEGRA_EMC_MRW_DEV1	2
#define TEGRA_EMC_MRW_DEV2	1

#define MC_EMEM_DEV_SIZE_MASK	0xF
#define MC_EMEM_DEV_SIZE_SHIFT	16

#define CLK_RST_CONTROLLER_CLK_SOURCE_EMC	0x19c
#define EMC_CLK_EMC_2X_CLK_SRC_SHIFT		29
#define EMC_CLK_EMC_2X_CLK_SRC_MASK		\
	(0x7 << EMC_CLK_EMC_2X_CLK_SRC_SHIFT)
#define EMC_CLK_SOURCE_PLLM			0x0
#define EMC_CLK_SOURCE_PLLM_LJ			0x4
#define EMC_CLK_SOURCE_PLLMB			0x6
#define EMC_CLK_SOURCE_PLLMB_LJ			0x5
#define EMC_CLK_FORCE_CC_TRIGGER		(0x1 << 27)
#define	EMC_CLK_MC_EMC_SAME_FREQ		(0x1 << 16)
#define EMC_CLK_EMC_2X_CLK_DIVISOR_SHIFT	0
#define EMC_CLK_EMC_2X_CLK_DIVISOR_MASK		\
	(0xff << EMC_CLK_EMC_2X_CLK_DIVISOR_SHIFT)

#define CLK_RST_CONTROLLER_CLK_OUT_ENB_X_SET	0x284
#define CLK_RST_CONTROLLER_CLK_OUT_ENB_X_CLR	0x288

#define CLK_OUT_ENB_X_CLK_ENB_EMC_DLL		(1 << 14)

#define CLK_RST_CONTROLLER_CLK_SOURCE_EMC_DLL	0x664
#define DLL_CLK_EMC_DLL_CLK_SRC_SHIFT		29
#define DLL_CLK_EMC_DLL_CLK_SRC_MASK		\
	(0x7 << DLL_CLK_EMC_DLL_CLK_SRC_SHIFT)
#define DLL_CLK_EMC_DLL_DDLL_CLK_SEL_SHIFT	10
#define DLL_CLK_EMC_DLL_DDLL_CLK_SEL_MASK	\
	(0x3 << DLL_CLK_EMC_DLL_DDLL_CLK_SEL_SHIFT)
#define PLLM_VCOA				0
#define PLLM_VCOB				1
#define EMC_DLL_SWITCH_OUT			2
#define DLL_CLK_EMC_DLL_CLK_DIVISOR_SHIFT	0
#define DLL_CLK_EMC_DLL_CLK_DIVISOR_MASK	\
	(0xff << DLL_CLK_EMC_DLL_CLK_DIVISOR_SHIFT)

#define BURST_PERCH_LIST			\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_MRW10),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_MRW10),	\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_MRW11),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_MRW11),	\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_MRW12),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_MRW12),	\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_MRW13),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_MRW13),	\


#define BURST_REG_LIST							\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RC),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RFC),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RFCPB),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_REFCTRL2),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RFC_SLR),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RAS),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RP),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_R2W),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_W2R),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_R2P),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_W2P),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_R2R),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TPPD),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CCDMW),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RD_RCD),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WR_RCD),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RRD),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_REXT),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WEXT),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WDV_CHK),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WDV),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WSV),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WEV),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WDV_MASK),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WS_DURATION),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_WE_DURATION),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_QUSE),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_QUSE_WIDTH),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_IBDLY),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_OBDLY),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_EINPUT),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_MRW6),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_EINPUT_DURATION),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PUTERM_EXTRA),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PUTERM_WIDTH),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_QRST),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_QSAFE),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RDV),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RDV_MASK),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RDV_EARLY),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RDV_EARLY_MASK),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_REFRESH),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_BURST_REFRESH_NUM),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PRE_REFRESH_REQ_CNT),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PDEX2WR),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PDEX2RD),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PCHG2PDEN),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_ACT2PDEN),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_AR2PDEN),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_RW2PDEN),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CKE2PDEN),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PDEX2CKE),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PDEX2MRR),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TXSR),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TXSRDLL),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TCKE),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TCKESR),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TPD),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TFAW),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRPAB),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TCLKSTABLE),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TCLKSTOP),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_MRW7),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TREFBW),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_ODT_WRITE),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_FBIO_CFG5),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_FBIO_CFG7),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CFG_DIG_DLL),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CFG_DIG_DLL_PERIOD),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_RXRT),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CFG_PIPE_1),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CFG_PIPE_2),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK0_4),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK0_5),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK1_4),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK1_5),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_MRW8),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK1_4),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK1_5),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK0_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK0_4), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK0_5), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK1_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK1_4), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQS_RANK1_5), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_LONG_CMD_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_LONG_CMD_1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_LONG_CMD_2),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_LONG_CMD_3),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_LONG_CMD_4),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_SHORT_CMD_0),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_SHORT_CMD_1),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_SHORT_CMD_2),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE0_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE1_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE2_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE3_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE4_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE5_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE6_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE7_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD0_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD1_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD2_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD3_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE0_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE1_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE2_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE3_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE4_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE5_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE6_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE7_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD0_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD1_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD2_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD2_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD2_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD2_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD3_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD3_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD3_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_CMD3_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TXDSRVTTGEN),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_FDPD_CTRL_DQ),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_FDPD_CTRL_CMD),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_FBIO_SPARE),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_ZCAL_INTERVAL),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_ZCAL_WAIT_CNT),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_MRS_WAIT_CNT),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_MRS_WAIT_CNT2),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_AUTO_CAL_CHANNEL),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_DLL_CFG_0),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_DLL_CFG_1),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_AUTOCAL_CFG_COMMON),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_ZCTRL),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CFG),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CFG_PIPE),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_DYN_SELF_REF_CONTROL),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_QPOP),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_DQS_BRLSHFT_0),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_DQS_BRLSHFT_1),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CMD_BRLSHFT_2),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CMD_BRLSHFT_3),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_PAD_CFG_CTRL),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DATA_PAD_RX_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_CMD_PAD_RX_CTRL),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DATA_RX_TERM_MODE),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_CMD_RX_TERM_MODE),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_CMD_PAD_TX_CTRL),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DATA_PAD_TX_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_COMMON_PAD_TX_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_VTTGEN_CTRL_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_VTTGEN_CTRL_1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_VTTGEN_CTRL_2),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_BRICK_CTRL_RFU1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_CMD_BRICK_CTRL_FDPD),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_BRICK_CTRL_RFU2),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DATA_BRICK_CTRL_FDPD),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_BG_BIAS_CTRL_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CFG_3),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_PWRD_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_PWRD_1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_PWRD_2),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_PWRD_3),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_PWRD_4),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_PWRD_5),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_CONFIG_SAMPLE_DELAY),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_SEL_CLK_SRC_0),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_SEL_CLK_SRC_1),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_SEL_CLK_SRC_2),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_SEL_CLK_SRC_3),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_SEL_CLK_SRC_4),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_TX_SEL_CLK_SRC_5),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_BYPASS),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_PWRD_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_PWRD_1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_DDLL_PWRD_2),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_CMD_CTRL_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_CMD_CTRL_1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_CMD_CTRL_2),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_TIMING_0),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_DVFS),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_CTRL_1),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_RDV),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_QPOP),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_RDV_MASK),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_MRW14),				\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_QSAFE),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TR_QRST),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_CTRL),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_SETTLE),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_VREF_SETTLE),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_CA_FINE_CTRL),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_CA_CTRL_MISC),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_CA_CTRL_MISC1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_CA_VREF_CTRL),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_QUSE_CORS_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_QUSE_FINE_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_QUSE_CTRL_MISC),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_QUSE_VREF_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_READ_FINE_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_READ_CTRL_MISC),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_READ_VREF_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_WRITE_FINE_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_WRITE_CTRL_MISC),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_WRITE_VREF_CTRL),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_TRAINING_MPC),			\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_MRW15)


#define TRIM_PERCH_REG_LIST					\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_CMD_BRLSHFT_0),		\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_CMD_BRLSHFT_1),		\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_DATA_BRLSHFT_0),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_DATA_BRLSHFT_0),	\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_DATA_BRLSHFT_1),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_DATA_BRLSHFT_1),	\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_QUSE_BRLSHFT_0),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_QUSE_BRLSHFT_1),	\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_QUSE_BRLSHFT_2),	\
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_QUSE_BRLSHFT_3)

#define TRIM_REG_LIST							\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK0_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_LONG_DQS_RANK1_3), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE2_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE2_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE2_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE3_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE3_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE3_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE4_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE4_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE4_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE5_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE5_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE5_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE6_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE6_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE6_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE7_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE7_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK0_BYTE7_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE2_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE2_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE2_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE3_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE3_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE3_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE4_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE4_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE4_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE5_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE5_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE5_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE6_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE6_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE6_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE7_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE7_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_DDLL_SHORT_DQ_RANK1_BYTE7_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_VREF_DQS_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_VREF_DQS_1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_VREF_DQ_0),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_IB_VREF_DQ_1),		\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK0_0),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK0_1),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK0_2),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK0_3),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK0_4),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK0_5),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK1_0),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK1_1),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK1_2),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_LONG_DQ_RANK1_3),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE2_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE2_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE2_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE3_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE3_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE3_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE4_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE4_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE4_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE5_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE5_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE5_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE6_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE6_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE6_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE7_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE7_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_BYTE7_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD2_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD2_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD2_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD3_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD3_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK0_CMD3_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE0_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE0_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE0_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE1_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE1_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE1_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE2_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE2_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE2_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE3_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE3_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE3_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE4_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE4_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE4_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE5_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE5_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE5_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE6_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE6_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE6_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE7_0), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE7_1), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_OB_DDLL_SHORT_DQ_RANK1_BYTE7_2), \
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK0_0),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK0_1),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK0_2),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK0_3),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK1_0),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK1_1),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK1_2),	\
	DEFINE_REG(TEGRA_EMC_BASE, EMC_PMACRO_QUSE_DDLL_RANK1_3)

#define BURST_MC_REG_LIST						\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_CFG),			\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_OUTSTANDING_REQ),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_REFPB_HP_CTRL),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_REFPB_BANK_CTRL),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_RCD),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_RP),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_RC),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_RAS),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_FAW),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_RRD),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_RAP2PRE),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_WAP2PRE),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_R2R),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_W2W),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_R2W),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_CCDMW),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_W2R),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_TIMING_RFCPB),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DA_TURNS),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DA_COVERS),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_MISC0),			\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_MISC1),			\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_MISC2),			\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_RING1_THROTTLE),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_CTRL),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_1),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_2),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_3),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_4),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_5),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_6),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_EMEM_ARB_DHYST_TIMEOUT_UTIL_7),	\

#define BURST_UP_DOWN_REG_LIST						\
	DEFINE_REG(TEGRA_MC_BASE, MC_MLL_MPCORER_PTSA_RATE),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_FTOP_PTSA_RATE),			\
	DEFINE_REG(TEGRA_MC_BASE, MC_PTSA_GRANT_DECREMENT),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_XUSB_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_XUSB_1),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_TSEC_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_SDMMCA_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_SDMMCAA_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_SDMMC_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_SDMMCAB_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_PPCS_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_PPCS_1),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_MPCORE_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_HC_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_HC_1),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_AVPC_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_GPU_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_GPU2_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_NVENC_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_NVDEC_0),	\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_VIC_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_VI2_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_ISP2_0),		\
	DEFINE_REG(TEGRA_MC_BASE, MC_LATENCY_ALLOWANCE_ISP2_1)

#define VREF_PERCH_REG_LIST						\
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_TRAINING_OPT_DQS_IB_VREF_RANK0), \
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_TRAINING_OPT_DQS_IB_VREF_RANK0), \
	DEFINE_REG(TEGRA_EMC0_BASE, EMC_TRAINING_OPT_DQS_IB_VREF_RANK1), \
	DEFINE_REG(TEGRA_EMC1_BASE, EMC_TRAINING_OPT_DQS_IB_VREF_RANK1), \

#define DEFINE_REG(base, reg) ((base) ? (IO_ADDRESS((base)) + (reg)) : NULL)

/*
 * Currently these are the IO virtual mapped addresses. Once iomap.h is removed
 * and the DT is used for loading the register addresses these will turn into
 * register offset by default and will be updated with the real address during
 * init.
 */
static void __iomem *burst_reg_off[] = {
	BURST_REG_LIST
};
static void __iomem *burst_perch_reg_off[] = {
	BURST_PERCH_LIST
};
static void __iomem *vref_reg_off[] = {
	VREF_PERCH_REG_LIST
};
static void __iomem *trim_reg_off[] = {
	TRIM_REG_LIST
};
static void __iomem *trim_perch_reg_off[] = {
	TRIM_PERCH_REG_LIST
};

/*
 * The MC registers that the clock change will modify.
 */
static void __iomem *la_scale_off_regs[] = {
	BURST_UP_DOWN_REG_LIST
};
static void __iomem *burst_mc_reg_off[] = {
	BURST_MC_REG_LIST
};

#undef DEFINE_REG

#define DEFINE_REG(base, reg)	reg##_INDEX
enum {
	BURST_REG_LIST
};

enum {
	BURST_MC_REG_LIST
};

enum {
	TRIM_REG_LIST
};


struct emc_sel {
	struct clk	*input;
	u32		value;
	unsigned long	input_rate;
};
static struct emc_sel tegra_emc_clk_sel[TEGRA_EMC_TABLE_MAX_SIZE];
static struct emc_sel tegra_emc_clk_sel_b[TEGRA_EMC_TABLE_MAX_SIZE];
static struct tegra21_emc_table start_timing;
static const struct tegra21_emc_table *emc_timing;
static unsigned long dram_over_temp_state = DRAM_OVER_TEMP_NONE;

static ktime_t clkchange_time;
static int clkchange_delay = 100;

static const struct tegra21_emc_table *tegra_emc_table;
static const struct tegra21_emc_table *tegra_emc_table_derated;
static int tegra_emc_table_size;

static u32 dram_dev_num;
static u32 dram_type = -1;

static struct clk *emc;

static struct {
	cputime64_t time_at_clock[TEGRA_EMC_TABLE_MAX_SIZE];
	int last_sel;
	u64 last_update;
	u64 clkchange_count;
	spinlock_t spinlock;
} emc_stats;

static DEFINE_SPINLOCK(emc_access_lock);

static inline void emc_writel(u32 val, unsigned long addr)
{
	emc_cc_dbg(REGS, "reg write 0x%08x => 0x%p\n", val, emc_base + addr);
	writel(val, emc_base + addr);
}

static inline void emc1_writel(u32 val, unsigned long addr)
{
	writel(val, emc1_base + addr);
}

static inline u32 emc_readl(unsigned long addr)
{
	u32 val;

	val = readl(emc_base + addr);
	emc_cc_dbg(REGS, "reg read 0x%p => 0x%08x\n", emc_base + addr, val);

	return val;
}

static inline u32 emc1_readl(unsigned long addr)
{
	u32 val;

	val = readl(emc1_base + addr);
	emc_cc_dbg(REGS, "reg read (emc1) 0x%p => 0x%08x\n",
		   emc_base + addr, val);

	return val;
}

static inline void mc_writel(u32 val, unsigned long addr)
{
	writel(val, mc_base + addr);
}

static inline u32 mc_readl(unsigned long addr)
{
	return readl(mc_base + addr);
}

static int ccfifo_index;
static inline void ccfifo_writel(u32 val, unsigned long addr, u32 delay)
{
	/* Index into CCFIFO - for keeping track of how many writes we
	 * generate. */

	emc_cc_dbg(CCFIFO, "[%d] (%u) 0x%08x => 0x%03lx\n",
		   ccfifo_index, delay, val, addr);
	ccfifo_index++;

	writel(val, emc_base + EMC_CCFIFO_DATA);
	writel((addr & 0xffff) | ((delay & 0x7fff) << 16) | (1 << 31),
	       emc_base + EMC_CCFIFO_ADDR);
}

static inline u32 disable_emc_sel_dpd_ctrl(u32 inreg)
{
	u32 mod_reg = inreg;
	mod_reg &= ~(EMC_SEL_DPD_CTRL_DATA_SEL_DPD_EN);
	mod_reg &= ~(EMC_SEL_DPD_CTRL_ODT_SEL_DPD_EN);
	if (dram_type == DRAM_TYPE_DDR3)
		mod_reg &= ~(EMC_SEL_DPD_CTRL_RESET_SEL_DPD_EN);
	mod_reg &= ~(EMC_SEL_DPD_CTRL_CA_SEL_DPD_EN);
	mod_reg &= ~(EMC_SEL_DPD_CTRL_CLK_SEL_DPD_EN);
	return mod_reg;
}

static int last_round_idx;
static inline int get_start_idx(unsigned long rate)
{
	if (tegra_emc_table[last_round_idx].rate == rate)
		return last_round_idx;
	return 0;
}
static void emc_last_stats_update(int last_sel)
{
	unsigned long flags;
	u64 cur_jiffies = get_jiffies_64();

	spin_lock_irqsave(&emc_stats.spinlock, flags);

	if (emc_stats.last_sel < TEGRA_EMC_TABLE_MAX_SIZE)
		emc_stats.time_at_clock[emc_stats.last_sel] =
			emc_stats.time_at_clock[emc_stats.last_sel] +
			(cur_jiffies - emc_stats.last_update);

	emc_stats.last_update = cur_jiffies;

	if (last_sel < TEGRA_EMC_TABLE_MAX_SIZE) {
		emc_stats.clkchange_count++;
		emc_stats.last_sel = last_sel;
	}
	spin_unlock_irqrestore(&emc_stats.spinlock, flags);
}

/*
 * Necessary for the dram_timing_regs array. These are not actually registers.
 * They are just used for computing values to put into the real timing
 * registers.
 */
static const struct tegra21_emc_table *get_timing_from_freq(unsigned long freq)
{
	int i;

	for (i = 0; i < tegra_emc_table_size; i++)
		if (tegra_emc_table[i].rate == freq)
			return &tegra_emc_table[i];

	return NULL;
}

static int wait_for_update(u32 status_reg, u32 bit_mask, bool updated_state,
			   int chan)
{
	int i, err = -ETIMEDOUT;
	int old_dbg_mask;
	u32 reg;

	emc_cc_dbg(REGS, "Polling 0x%08x (chan=%d) for 0x%08x => 0x%08x\n",
		   status_reg, chan, bit_mask, updated_state);

	/* Turn off REGS to hide a potentially huge number of prints. */
	old_dbg_mask = emc_dbg_mask;
	emc_dbg_mask &= ~REGS;

	for (i = 0; i < EMC_STATUS_UPDATE_TIMEOUT; i++) {
		reg = chan ? emc1_readl(status_reg) : emc_readl(status_reg);
		if (!!(reg & bit_mask) == updated_state) {
			err = 0;
			goto done;
		}
		udelay(1);
	}

done:
	emc_dbg_mask = old_dbg_mask;
	emc_cc_dbg(REGS, "Polling cycles: %d\n", i);
	return err;
}

static inline void emc_timing_update(int dual_chan)
{
	int err = 0;

	emc_writel(0x1, EMC_TIMING_CONTROL);
	err |= wait_for_update(EMC_EMC_STATUS,
			       EMC_EMC_STATUS_TIMING_UPDATE_STALLED, false, 0);
	if (dual_chan)
		err |= wait_for_update(EMC_EMC_STATUS,
			       EMC_EMC_STATUS_TIMING_UPDATE_STALLED, false, 1);
	if (err) {
		pr_err("%s: timing update error: %d", __func__, err);
		BUG();
	}
}

static inline void set_over_temp_timing(
	const struct tegra21_emc_table *next_timing, unsigned long state)
{
#define REFRESH_X2      1
#define REFRESH_X4      2
#define REFRESH_SPEEDUP(val, speedup)					\
	do {					\
		val = ((val) & 0xFFFF0000) |				\
			(((val) & 0xFFFF) >> (speedup));		\
	} while (0)

	u32 ref = next_timing->burst_regs[EMC_REFRESH_INDEX];
	u32 pre_ref = next_timing->burst_regs[EMC_PRE_REFRESH_REQ_CNT_INDEX];
	u32 dsr_cntrl = next_timing->burst_regs[EMC_DYN_SELF_REF_CONTROL_INDEX];

	switch (state) {
	case DRAM_OVER_TEMP_NONE:
		break;
	case DRAM_OVER_TEMP_REFRESH_X2:
		REFRESH_SPEEDUP(ref, REFRESH_X2);
		REFRESH_SPEEDUP(pre_ref, REFRESH_X2);
		REFRESH_SPEEDUP(dsr_cntrl, REFRESH_X2);
		break;
	case DRAM_OVER_TEMP_REFRESH_X4:
	case DRAM_OVER_TEMP_THROTTLE:
		REFRESH_SPEEDUP(ref, REFRESH_X4);
		REFRESH_SPEEDUP(pre_ref, REFRESH_X4);
		REFRESH_SPEEDUP(dsr_cntrl, REFRESH_X4);
		break;
	default:
	WARN(1, "%s: Failed to set dram over temp state %lu\n",
		__func__, state);
	return;
	}

	__raw_writel(ref, burst_reg_off[EMC_REFRESH_INDEX]);
	__raw_writel(pre_ref, burst_reg_off[EMC_PRE_REFRESH_REQ_CNT_INDEX]);
	__raw_writel(dsr_cntrl, burst_reg_off[EMC_DYN_SELF_REF_CONTROL_INDEX]);
	wmb();
}

static inline void overwrite_mrs_wait_cnt(
	const struct tegra21_emc_table *next_timing,
	bool zcal_long)
{
	u32 reg;
	u32 cnt = 512;

	/* For ddr3 when DLL is re-started: overwrite EMC DFS table settings
	   for MRS_WAIT_LONG with maximum of MRS_WAIT_SHORT settings and
	   expected operation length. Reduce the latter by the overlapping
	   zq-calibration, if any */
	if (zcal_long)
		cnt -= dram_dev_num * 256;

	reg = (next_timing->burst_regs[EMC_MRS_WAIT_CNT_INDEX] &
		EMC_MRS_WAIT_CNT_SHORT_WAIT_MASK) >>
		EMC_MRS_WAIT_CNT_SHORT_WAIT_SHIFT;
	if (cnt < reg)
		cnt = reg;

	reg = (next_timing->burst_regs[EMC_MRS_WAIT_CNT_INDEX] &
		(~EMC_MRS_WAIT_CNT_LONG_WAIT_MASK));
	reg |= (cnt << EMC_MRS_WAIT_CNT_LONG_WAIT_SHIFT) &
		EMC_MRS_WAIT_CNT_LONG_WAIT_MASK;

	emc_writel(reg, EMC_MRS_WAIT_CNT);
}

static inline void do_clock_change(u32 clk_setting)
{
	int err;

	mc_readl(MC_EMEM_ADR_CFG);	/* completes prev writes */
	emc_readl(EMC_INTSTATUS);

	writel(clk_setting, clk_base + emc->reg);
	readl(clk_base + emc->reg);     /* completes prev write */

	err = wait_for_update(EMC_INTSTATUS,
			      EMC_INTSTATUS_CLKCHANGE_COMPLETE, true, 0);
	if (err) {
		pr_err("%s: clock change completion error: %d", __func__, err);
		BUG();
	}
}

static inline void emc_set_shadow_bypass(int set)
{
	u32 emc_dbg = emc_readl(EMC_DBG);

	emc_cc_dbg(ACTIVE_EN, "Setting write mux: %s\n",
		   set ? "ACTIVE" : "SHADOW");

	if (set)
		emc_writel(emc_dbg | EMC_DBG_WRITE_MUX_ACTIVE, EMC_DBG);
	else
		emc_writel(emc_dbg & ~EMC_DBG_WRITE_MUX_ACTIVE, EMC_DBG);
}

static inline u32 get_dll_state(const struct tegra21_emc_table *next_timing)
{
	bool next_dll_enabled;

	next_dll_enabled = !(next_timing->emc_emrs & 0x1);
	if (next_dll_enabled)
		return DLL_ON;
	else
		return DLL_OFF;
}

/*
 * This function computes the division of two fixed point numbers which have
 * three decimal places of precision. The result is then ceil()ed and converted
 * to a regular integer.
 */
static inline u32 div_o3(u32 a, u32 b)
{
	u32 result = a / b;

	if ((b * result) < a)
		return result + 1;
	else
		return result;
}

/*
 * Source clock period is in picoseconds. Returns the ramp down wait time in
 * picoseconds.
 */
noinline u32 do_dvfs_power_ramp_down(u32 clk, int flip_backward,
			     const struct tegra21_emc_table *last_timing,
			     const struct tegra21_emc_table *next_timing)
{
	u32 ramp_down_wait = 0;
	u32 pmacro_cmd_pad;
	u32 pmacro_dq_pad;
	u32 pmacro_rfu1;
	u32 pmacro_cfg5;
	u32 pmacro_common_tx;
	u32 seq_wait;

	emc_cc_dbg(PRAMP_DN, "flip_backward = %d\n", flip_backward);

	if (flip_backward) {
		pmacro_cmd_pad   = next_timing->
			burst_regs[EMC_PMACRO_CMD_PAD_TX_CTRL_INDEX];
		pmacro_dq_pad    = next_timing->
			burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX];
		pmacro_rfu1      = next_timing->
			burst_regs[EMC_PMACRO_BRICK_CTRL_RFU1_INDEX];
		pmacro_cfg5      = next_timing->
			burst_regs[EMC_FBIO_CFG5_INDEX];
		pmacro_common_tx = next_timing->
			burst_regs[EMC_PMACRO_COMMON_PAD_TX_CTRL_INDEX];
	} else {
		pmacro_cmd_pad   = last_timing->
			burst_regs[EMC_PMACRO_CMD_PAD_TX_CTRL_INDEX];
		pmacro_dq_pad    = last_timing->
			burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX];
		pmacro_rfu1      = last_timing->
			burst_regs[EMC_PMACRO_BRICK_CTRL_RFU1_INDEX];
		pmacro_cfg5      = last_timing->
			burst_regs[EMC_FBIO_CFG5_INDEX];
		pmacro_common_tx = last_timing->
			burst_regs[EMC_PMACRO_COMMON_PAD_TX_CTRL_INDEX];
	}

	pmacro_cmd_pad |= EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_DRVFORCEON;

	ccfifo_writel(pmacro_cmd_pad, EMC_PMACRO_CMD_PAD_TX_CTRL, 0);
	ccfifo_writel(pmacro_cfg5 | EMC_FBIO_CFG5_CMD_TX_DIS, EMC_FBIO_CFG5,
		      12);
	ramp_down_wait = 12 * clk;

	seq_wait = (100000 / clk) + 1;

	if (clk < (1000000 / DVFS_FGCG_HIGH_SPEED_THRESHOLD)) {
		emc_cc_dbg(PRAMP_DN, "clk < FGCG_HIGH_SPEED_THRESHOLD;\n");
		emc_cc_dbg(PRAMP_DN, "  %u vs %u\n", clk,
			   1000000 / DVFS_FGCG_HIGH_SPEED_THRESHOLD);

		if (clk < (1000000 / IOBRICK_DCC_THRESHOLD)) {
			emc_cc_dbg(PRAMP_DN, "clk < IOBRICK_DCC_THRESHOLD;\n");
			emc_cc_dbg(PRAMP_DN, "  %u vs %u\n", clk,
			   1000000 / IOBRICK_DCC_THRESHOLD);

			pmacro_cmd_pad &=
				~(EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_CMD_TX_E_DCC);
			pmacro_cmd_pad |=
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSP_TX_E_DCC |
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSN_TX_E_DCC;
			ccfifo_writel(pmacro_cmd_pad,
				      EMC_PMACRO_CMD_PAD_TX_CTRL, seq_wait);
			ramp_down_wait += 100000;

			pmacro_dq_pad &=
			      ~(EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_CMD_TX_E_DCC);
			pmacro_dq_pad |=
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSP_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSN_TX_E_DCC;
			ccfifo_writel(pmacro_dq_pad,
				      EMC_PMACRO_DATA_PAD_TX_CTRL, 0);
			ccfifo_writel(pmacro_rfu1 & ~0x01120112,
				      EMC_PMACRO_BRICK_CTRL_RFU1, 0);
		} else {
			emc_cc_dbg(PRAMP_DN, "clk > IOBRICK_DCC_THRESHOLD\n");
			ccfifo_writel(pmacro_rfu1 & ~0x01120112,
				      EMC_PMACRO_BRICK_CTRL_RFU1, seq_wait);
			ramp_down_wait += 100000;
		}

		ccfifo_writel(pmacro_rfu1 & ~0x01bf01bf,
			      EMC_PMACRO_BRICK_CTRL_RFU1, seq_wait);
		ramp_down_wait += 100000;

		if (clk < (1000000 / IOBRICK_DCC_THRESHOLD)) {
			pmacro_cmd_pad &=
				~(EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_CMD_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSP_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSN_TX_E_DCC);
			ccfifo_writel(pmacro_cmd_pad,
				      EMC_PMACRO_CMD_PAD_TX_CTRL, seq_wait);
			ramp_down_wait += 100000;

			pmacro_dq_pad &=
			      ~(EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_CMD_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSP_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSN_TX_E_DCC);
			ccfifo_writel(pmacro_dq_pad,
				      EMC_PMACRO_DATA_PAD_TX_CTRL, 0);
			ccfifo_writel(pmacro_rfu1 & ~0x07ff07ff,
				      EMC_PMACRO_BRICK_CTRL_RFU1, 0);
		} else {
			ccfifo_writel(pmacro_rfu1 & ~0x07ff07ff,
				      EMC_PMACRO_BRICK_CTRL_RFU1, seq_wait);
			ramp_down_wait += 100000;
		}
	} else {
		emc_cc_dbg(PRAMP_DN, "clk > FGCG_HIGH_SPEED_THRESHOLD\n");
		ccfifo_writel(pmacro_rfu1 & ~0xffff07ff,
			      EMC_PMACRO_BRICK_CTRL_RFU1, seq_wait + 19);
		ramp_down_wait += 100000 + (20 * clk);
	}

	if (clk < (1000000 / DVFS_FGCG_MID_SPEED_THRESHOLD)) {
		emc_cc_dbg(PRAMP_DN, "clk < FGCG_MID_SPEED_THRESHOLD;\n");
		emc_cc_dbg(PRAMP_DN, "  %u vs %u\n", clk,
			   1000000 / DVFS_FGCG_MID_SPEED_THRESHOLD);

		ramp_down_wait += 100000;
		ccfifo_writel(pmacro_common_tx & ~0x5,
			      EMC_PMACRO_COMMON_PAD_TX_CTRL, seq_wait);
		ramp_down_wait += 100000;
		ccfifo_writel(pmacro_common_tx & ~0xf,
			      EMC_PMACRO_COMMON_PAD_TX_CTRL, seq_wait);
		ramp_down_wait += 100000;
		ccfifo_writel(0, 0, seq_wait);
		ramp_down_wait += 100000;
	} else {
		emc_cc_dbg(PRAMP_DN, "clk > FGCG_MID_SPEED_THRESHOLD\n");
		ccfifo_writel(pmacro_common_tx & ~0xf,
			      EMC_PMACRO_COMMON_PAD_TX_CTRL, seq_wait);
	}

	return ramp_down_wait;
}

/*
 * Similar to do_dvfs_power_ramp_down() except this does the power ramp up.
 */
noinline u32 do_dvfs_power_ramp_up(u32 clk, int flip_backward,
				   const struct tegra21_emc_table *last_timing,
				   const struct tegra21_emc_table *next_timing)
{
	u32 pmacro_cmd_pad;
	u32 pmacro_dq_pad;
	u32 pmacro_rfu1;
	u32 pmacro_cfg5;
	u32 pmacro_common_tx;
	u32 ramp_up_wait = 0;

	if (flip_backward) {
		pmacro_cmd_pad   = last_timing->
			burst_regs[EMC_PMACRO_CMD_PAD_TX_CTRL_INDEX];
		pmacro_dq_pad    = last_timing->
			burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX];
		pmacro_rfu1      = last_timing->
			burst_regs[EMC_PMACRO_BRICK_CTRL_RFU1_INDEX];
		pmacro_cfg5      = last_timing->burst_regs[EMC_FBIO_CFG5_INDEX];
		pmacro_common_tx = last_timing->
			burst_regs[EMC_PMACRO_COMMON_PAD_TX_CTRL_INDEX];
	} else {
		pmacro_cmd_pad   = next_timing->
			burst_regs[EMC_PMACRO_CMD_PAD_TX_CTRL_INDEX];
		pmacro_dq_pad    = next_timing->
			burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX];
		pmacro_rfu1      = next_timing->
			burst_regs[EMC_PMACRO_BRICK_CTRL_RFU1_INDEX];
		pmacro_cfg5      = next_timing->
			burst_regs[EMC_FBIO_CFG5_INDEX];
		pmacro_common_tx = next_timing->
			burst_regs[EMC_PMACRO_COMMON_PAD_TX_CTRL_INDEX];
	}
	pmacro_cmd_pad |= EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_DRVFORCEON;

	if (clk < 1000000 / DVFS_FGCG_MID_SPEED_THRESHOLD) {
		ccfifo_writel(pmacro_common_tx & 0xa,
			      EMC_PMACRO_COMMON_PAD_TX_CTRL, 0);
		ccfifo_writel(pmacro_common_tx & 0xf,
			      EMC_PMACRO_COMMON_PAD_TX_CTRL,
			      (100000 / clk) + 1);
		ramp_up_wait += 100000;
	} else {
		ccfifo_writel(pmacro_common_tx | 0x8,
			      EMC_PMACRO_COMMON_PAD_TX_CTRL, 0);
	}

	if (clk < 1000000 / DVFS_FGCG_HIGH_SPEED_THRESHOLD) {
		if (clk < 1000000 / IOBRICK_DCC_THRESHOLD) {
			pmacro_cmd_pad |=
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSP_TX_E_DCC |
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSN_TX_E_DCC;
			pmacro_cmd_pad &=
				~(EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_CMD_TX_E_DCC);
			ccfifo_writel(pmacro_cmd_pad,
				      EMC_PMACRO_CMD_PAD_TX_CTRL,
				      (100000 / clk) + 1);
			ramp_up_wait += 100000;

			pmacro_dq_pad |=
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSP_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSN_TX_E_DCC;
			pmacro_dq_pad &=
			       ~(EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_TX_E_DCC |
				 EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_CMD_TX_E_DCC);
			ccfifo_writel(pmacro_dq_pad,
				      EMC_PMACRO_DATA_PAD_TX_CTRL, 0);
			ccfifo_writel(pmacro_rfu1 & 0xfe40fe40,
				      EMC_PMACRO_BRICK_CTRL_RFU1, 0);
		} else {
			ccfifo_writel(pmacro_rfu1 & 0xfe40fe40,
				      EMC_PMACRO_BRICK_CTRL_RFU1,
				      (100000 / clk) + 1);
			ramp_up_wait += 100000;
		}

		ccfifo_writel(pmacro_rfu1 & 0xfeedfeed,
			      EMC_PMACRO_BRICK_CTRL_RFU1, (100000 / clk) + 1);
		ramp_up_wait += 100000;

		if (clk < 1000000 / IOBRICK_DCC_THRESHOLD) {
			pmacro_cmd_pad |=
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSP_TX_E_DCC |
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSN_TX_E_DCC |
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_E_DCC |
				EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_CMD_TX_E_DCC;
			ccfifo_writel(pmacro_cmd_pad,
				      EMC_PMACRO_CMD_PAD_TX_CTRL,
				      (100000 / clk) + 1);
			ramp_up_wait += 100000;

			pmacro_dq_pad |=
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSP_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSN_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_TX_E_DCC |
				EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_CMD_TX_E_DCC;
			ccfifo_writel(pmacro_dq_pad,
				      EMC_PMACRO_DATA_PAD_TX_CTRL, 0);
			ccfifo_writel(pmacro_rfu1,
				      EMC_PMACRO_BRICK_CTRL_RFU1, 0);
		} else {
			ccfifo_writel(pmacro_rfu1,
				      EMC_PMACRO_BRICK_CTRL_RFU1,
				      (100000 / clk) + 1);
			ramp_up_wait += 100000;
		}

		ccfifo_writel(pmacro_cfg5 & ~EMC_FBIO_CFG5_CMD_TX_DIS,
			      EMC_FBIO_CFG5, (100000 / clk) + 10);
		ramp_up_wait += 100000 + (10 * clk);
	} else if (clk < 1000000 / DVFS_FGCG_MID_SPEED_THRESHOLD) {
		ccfifo_writel(pmacro_rfu1 | 0x06000600,
			      EMC_PMACRO_BRICK_CTRL_RFU1, (100000 / clk) + 1);
		ccfifo_writel(pmacro_cfg5 & ~EMC_FBIO_CFG5_CMD_TX_DIS,
			      EMC_FBIO_CFG5, (100000 / clk) + 10);
		ramp_up_wait += 100000 + 10 * clk;
	} else {
		ccfifo_writel(pmacro_rfu1 | 0x00000600,
			      EMC_PMACRO_BRICK_CTRL_RFU1, 0);
		ccfifo_writel(pmacro_cfg5 & ~EMC_FBIO_CFG5_CMD_TX_DIS,
			      EMC_FBIO_CFG5, 12);
		ramp_up_wait += 12 * clk;
	}

	pmacro_cmd_pad &= ~EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_DRVFORCEON;
	ccfifo_writel(pmacro_cmd_pad, EMC_PMACRO_CMD_PAD_TX_CTRL, 5);

	return ramp_up_wait;
}

/*
 * Change the DLL's input clock. Used during the DLL prelock sequence.
 */
noinline void change_dll_src(const struct tegra21_emc_table *next_timing,
			     u32 clksrc)
{
	u32 out_enb_x;
	u32 dll_setting = next_timing->dll_clk_src;
	u32 emc_clk_src;
	u32 emc_clk_div;

	out_enb_x = 0;
	emc_clk_src = (clksrc & EMC_CLK_EMC_2X_CLK_SRC_MASK) >>
		EMC_CLK_EMC_2X_CLK_SRC_SHIFT;
	emc_clk_div = (clksrc & EMC_CLK_EMC_2X_CLK_DIVISOR_MASK) >>
		EMC_CLK_EMC_2X_CLK_DIVISOR_SHIFT;

	dll_setting &= ~(DLL_CLK_EMC_DLL_CLK_SRC_MASK |
			 DLL_CLK_EMC_DLL_CLK_DIVISOR_MASK);
	dll_setting |= emc_clk_src << DLL_CLK_EMC_DLL_CLK_SRC_SHIFT;
	dll_setting |= emc_clk_div << DLL_CLK_EMC_DLL_CLK_DIVISOR_SHIFT;

	/* Low jitter and undivided are the same thing. */
	dll_setting &= ~DLL_CLK_EMC_DLL_DDLL_CLK_SEL_MASK;
	if (emc_clk_src == EMC_CLK_SOURCE_PLLMB_LJ)
		dll_setting |= (PLLM_VCOB <<
				DLL_CLK_EMC_DLL_DDLL_CLK_SEL_SHIFT);
	else if (emc_clk_src == EMC_CLK_SOURCE_PLLM_LJ)
		dll_setting |= (PLLM_VCOA <<
				DLL_CLK_EMC_DLL_DDLL_CLK_SEL_SHIFT);
	else
		dll_setting |= (EMC_DLL_SWITCH_OUT <<
				DLL_CLK_EMC_DLL_DDLL_CLK_SEL_SHIFT);

	/* Now program the clock source. */
	emc_cc_dbg(REGS, "clk source: 0x%08x => 0x%p\n", dll_setting,
		   clk_base + CLK_RST_CONTROLLER_CLK_SOURCE_EMC_DLL);
	writel(dll_setting, clk_base + CLK_RST_CONTROLLER_CLK_SOURCE_EMC_DLL);

	if (next_timing->clk_out_enb_x_0_clk_enb_emc_dll) {
		writel(CLK_OUT_ENB_X_CLK_ENB_EMC_DLL,
		       clk_base + CLK_RST_CONTROLLER_CLK_OUT_ENB_X_SET);
		emc_cc_dbg(REGS, "out_enb_x_set: 0x%08x => 0x%p\n",
			   CLK_OUT_ENB_X_CLK_ENB_EMC_DLL,
			   clk_base + CLK_RST_CONTROLLER_CLK_OUT_ENB_X_SET);
	} else {
		writel(CLK_OUT_ENB_X_CLK_ENB_EMC_DLL,
		       clk_base + CLK_RST_CONTROLLER_CLK_OUT_ENB_X_CLR);
		emc_cc_dbg(REGS, "out_enb_x_clr: 0x%08x => 0x%p\n",
			   CLK_OUT_ENB_X_CLK_ENB_EMC_DLL,
			   clk_base + CLK_RST_CONTROLLER_CLK_OUT_ENB_X_CLR);
	}
}

/*
 * Prelock the DLL.
 */
noinline u32 dll_prelock(const struct tegra21_emc_table *next_timing,
			 int dvfs_with_training, u32 clksrc)
{
	u32 emc_dig_dll_status;
	u32 dll_locked;
	u32 dll_out;
	u32 emc_cfg_dig_dll;
	u32 emc_dll_cfg_0;
	u32 emc_dll_cfg_1;
	u32 ddllcal_ctrl_start_trim_val;
	u32 dll_en;
	u32 dual_channel_lpddr4_case;
	u32 dll_priv_updated;

	emc_cc_dbg(PRELOCK, "Prelock starting; version: %d\n",
		   EMC_PRELOCK_VERSION);

	dual_channel_lpddr4_case =
		!!(emc_readl(EMC_FBIO_CFG7) & EMC_FBIO_CFG7_CH1_ENABLE) &
		!!(emc_readl(EMC_FBIO_CFG7) & EMC_FBIO_CFG7_CH0_ENABLE);

	emc_dig_dll_status = 0;
	dll_locked = 0;
	dll_out = 0;
	emc_cfg_dig_dll = 0;
	emc_dll_cfg_0 = 0;
	emc_dll_cfg_1 = 0;
	ddllcal_ctrl_start_trim_val = 0;
	dll_en = 0;

	emc_cc_dbg(PRELOCK, "Dual channel LPDDR4: %s\n",
		   dual_channel_lpddr4_case ? "yes" : "no");
	emc_cc_dbg(PRELOCK, "DLL clksrc: 0x%08x\n", clksrc);

	/* Step 1:
	 *   Configure the DLL for prelock.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 1\n");
	emc_cfg_dig_dll = emc_readl(EMC_CFG_DIG_DLL) &
		~EMC_CFG_DIG_DLL_CFG_DLL_LOCK_LIMIT_MASK;
	emc_cfg_dig_dll |= (3 << EMC_CFG_DIG_DLL_CFG_DLL_LOCK_LIMIT_SHIFT);
	emc_cfg_dig_dll &= ~EMC_CFG_DIG_DLL_CFG_DLL_EN;
	emc_cfg_dig_dll &= ~EMC_CFG_DIG_DLL_CFG_DLL_MODE_MASK;
	emc_cfg_dig_dll |= (3 << EMC_CFG_DIG_DLL_CFG_DLL_MODE_SHIFT);
	emc_cfg_dig_dll |= EMC_CFG_DIG_DLL_CFG_DLL_STALL_ALL_TRAFFIC;
	emc_cfg_dig_dll &= ~EMC_CFG_DIG_DLL_CFG_DLL_STALL_RW_UNTIL_LOCK;
	emc_cfg_dig_dll &= ~EMC_CFG_DIG_DLL_CFG_DLL_STALL_ALL_UNTIL_LOCK;

	emc_writel(emc_cfg_dig_dll, EMC_CFG_DIG_DLL);
	emc_writel(1, EMC_TIMING_CONTROL);

	/* Step 2:
	 *   Update timings.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 2\n");
	wait_for_update(EMC_EMC_STATUS,
			EMC_EMC_STATUS_TIMING_UPDATE_STALLED, 0, 0);
	if (dual_channel_lpddr4_case)
		wait_for_update(EMC_EMC_STATUS,
				EMC_EMC_STATUS_TIMING_UPDATE_STALLED, 0, 1);

	/* Step 3:
	 *   Poll channel(s) until DLL_EN is true.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 3\n");
	do {
		emc_cfg_dig_dll = emc_readl(EMC_CFG_DIG_DLL);
		dll_en = emc_cfg_dig_dll & EMC_CFG_DIG_DLL_CFG_DLL_EN;
	} while (dll_en == 1);

	if (dual_channel_lpddr4_case) {
		do {
			emc_cfg_dig_dll = emc1_readl(EMC_CFG_DIG_DLL);
			dll_en = emc_cfg_dig_dll & EMC_CFG_DIG_DLL_CFG_DLL_EN;
		} while (dll_en == 1);
	}

	/* Step 4:
	 *   Update DLL calibration filter.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 4\n");
	emc_dll_cfg_0 = next_timing->burst_regs[EMC_DLL_CFG_0_INDEX];

	emc_writel(emc_dll_cfg_0, EMC_DLL_CFG_0);

	if (next_timing->rate >= 400000 && next_timing->rate < 600000)
		ddllcal_ctrl_start_trim_val = 150;
	else if (next_timing->rate >= 600000 && next_timing->rate < 800000)
		ddllcal_ctrl_start_trim_val = 100;
	else if (next_timing->rate >= 800000 && next_timing->rate < 1000000)
		ddllcal_ctrl_start_trim_val = 70;
	else if (next_timing->rate >= 1000000 && next_timing->rate < 1200000)
		ddllcal_ctrl_start_trim_val = 30;
	else
		ddllcal_ctrl_start_trim_val = 20;

	emc_dll_cfg_1 = emc_readl(EMC_DLL_CFG_1);
	emc_dll_cfg_1 &= EMC_DLL_CFG_1_DDLLCAL_CTRL_START_TRIM_MASK;
	emc_dll_cfg_1 |= ddllcal_ctrl_start_trim_val;
	emc_writel(emc_dll_cfg_1, EMC_DLL_CFG_1);

	/* Step 8:
	 *   (Skipping some steps to get back inline with reference.)
	 *   Change the DLL clock source.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 8\n");
	change_dll_src(next_timing, clksrc);

	/* Step 9:
	 *   Enable the DLL and start the prelock state machine.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 9\n");
	emc_cfg_dig_dll = emc_readl(EMC_CFG_DIG_DLL);
	emc_cfg_dig_dll |= EMC_CFG_DIG_DLL_CFG_DLL_EN;
	emc_writel(emc_cfg_dig_dll, EMC_CFG_DIG_DLL);

	emc_timing_update(dual_channel_lpddr4_case ?
			  DUAL_CHANNEL : SINGLE_CHANNEL);

	do {
		emc_cfg_dig_dll = emc_readl(EMC_CFG_DIG_DLL);
		dll_en = emc_cfg_dig_dll & EMC_CFG_DIG_DLL_CFG_DLL_EN;
	} while (dll_en == 0);

	if (dual_channel_lpddr4_case) {
		do {
			emc_cfg_dig_dll = emc1_readl(EMC_CFG_DIG_DLL);
			dll_en = emc_cfg_dig_dll & EMC_CFG_DIG_DLL_CFG_DLL_EN;
		} while (dll_en == 0);
	}

	/* Step 10:
	 *   Wait for the DLL to lock.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 10\n");
	do {
		emc_dig_dll_status = emc_readl(EMC_DIG_DLL_STATUS);
		dll_locked = emc_dig_dll_status & EMC_DIG_DLL_STATUS_DLL_LOCK;
		dll_priv_updated = emc_dig_dll_status &
			EMC_DIG_DLL_STATUS_DLL_PRIV_UPDATED;
	} while (!dll_locked || !dll_priv_updated);

	/* Step 11:
	 *   Prelock training specific code - removed. Should it be ??
	 */

	/* Step 12:
	 *   Done! Return the dll prelock value.
	 */
	emc_cc_dbg(PRELOCK_STEPS, "Step 12\n");
	emc_dig_dll_status = emc_readl(EMC_DIG_DLL_STATUS);
	return emc_dig_dll_status & EMC_DIG_DLL_STATUS_DLL_OUT_MASK;
}

noinline void dll_disable(int channel_mode)
{
	u32 emc_cfg_dig_dll;

	emc_cfg_dig_dll = emc_readl(EMC_CFG_DIG_DLL);
	emc_cfg_dig_dll &= ~EMC_CFG_DIG_DLL_CFG_DLL_EN;
	emc_writel(emc_cfg_dig_dll, EMC_CFG_DIG_DLL);
	emc_timing_update(channel_mode);

	wait_for_update(EMC_CFG_DIG_DLL, EMC_CFG_DIG_DLL_CFG_DLL_EN, 0, 0);
	if (channel_mode == DUAL_CHANNEL)
		wait_for_update(EMC_CFG_DIG_DLL,
				EMC_CFG_DIG_DLL_CFG_DLL_EN, 0, 1);
}

/*
 */
noinline void emc_set_clock(const struct tegra21_emc_table *next_timing,
			    const struct tegra21_emc_table *last_timing,
			    int training, u32 clksrc)
{
	/*
	 * This is the timing table for the source frequency. It does _not_
	 * necessarily correspond to the actual timing values in the EMC at the
	 * moment. If the boot BCT differs from the table then this can happen.
	 * However, we need it for accessing the dram_timing_regs (which are not
	 * really registers) array for the current frequency.
	 */
	const struct tegra21_emc_table *fake_timing;

	u32 i, tmp;

	u32 cya_allow_ref_cc = 0, ref_b4_sref_en = 0, cya_issue_pc_ref = 0;

	u32 zqcal_before_cc_cutoff = 2400; /* In picoseconds */
	u32 ref_delay_mult;
	u32 ref_delay;
	s32 zq_latch_dvfs_wait_time;
	s32 tZQCAL_lpddr4_fc_adj;
	/* Scaled by x1000 */
	u32 tFC_lpddr4 = 1000 * next_timing->dram_timing_regs[T_FC_LPDDR4];
	/* u32 tVRCG_lpddr4 = next_timing->dram_timing_regs[T_FC_LPDDR4]; */
	u32 tZQCAL_lpddr4 = 1000000;

	u32 dram_type, dram_dev_num, shared_zq_resistor;
	u32 channel_mode;
	u32 is_lpddr3;

	u32 emc_cfg, emc_sel_dpd_ctrl, emc_cfg_reg;

	u32 emc_dbg;
	u32 emc_zcal_interval;
	u32 emc_zcal_wait_cnt_old;
	u32 emc_zcal_wait_cnt_new;
	u32 emc_dbg_active;
	u32 zq_op;
	u32 zcal_wait_time_clocks;
	u32 zcal_wait_time_ps;

	u32 emc_auto_cal_config;
	u32 auto_cal_en;

	u32 mr13_catr_enable;

	u32 ramp_up_wait = 0, ramp_down_wait = 0;

	/* In picoseconds. */
	u32 source_clock_period;
	u32 destination_clock_period;

	u32 emc_dbg_o;
	u32 emc_cfg_pipe_clk_o;
	u32 emc_pin_o;

	u32 mr13_flip_fspwr;
	u32 mr13_flip_fspop;

	u32 opt_zcal_en_cc;
	u32 opt_do_sw_qrst = 1;
	u32 opt_dvfs_mode;
	u32 opt_dll_mode;
	u32 opt_cc_short_zcal = 1;
	u32 opt_short_zcal = 1;
	u32 save_restore_clkstop_pd = 1;

	u32 prelock_dll_en = 0, dll_out;

	int next_push, next_dq_e_ivref, next_dqs_e_ivref;

	u64 emc_mrw6_ab = (u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_MRW6;
	u64 emc_mrw7_ab = (u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_MRW7;
	u64 emc_mrw8_ab = (u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_MRW8;
	u64 emc_mrw9_ab = (u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_MRW9;
	u64 emc_mrw10_ch0_ab = (u64)IO_ADDRESS(TEGRA_EMC0_BASE) + EMC_MRW10;
	u64 emc_mrw10_ch1_ab = (u64)IO_ADDRESS(TEGRA_EMC1_BASE) + EMC_MRW10;
	u64 emc_mrw11_ch0_ab = (u64)IO_ADDRESS(TEGRA_EMC0_BASE) + EMC_MRW11;
	u64 emc_mrw11_ch1_ab = (u64)IO_ADDRESS(TEGRA_EMC1_BASE) + EMC_MRW11;
	u64 emc_mrw12_ch0_ab = (u64)IO_ADDRESS(TEGRA_EMC0_BASE) + EMC_MRW12;
	u64 emc_mrw12_ch1_ab = (u64)IO_ADDRESS(TEGRA_EMC1_BASE) + EMC_MRW12;
	u64 emc_mrw13_ch0_ab = (u64)IO_ADDRESS(TEGRA_EMC0_BASE) + EMC_MRW13;
	u64 emc_mrw13_ch1_ab = (u64)IO_ADDRESS(TEGRA_EMC1_BASE) + EMC_MRW13;
	u64 emc_mrw14_ab = (u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_MRW14;
	u64 emc_mrw15_ab = (u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_MRW15;

	u64 emc_training_ctrl_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_TRAINING_CTRL;
	u64 emc_cfg_ab = (u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_CFG;
	u64 emc_mrs_wait_cnt_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_MRS_WAIT_CNT;
	u64 emc_zcal_wait_cnt_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_ZCAL_INTERVAL;
	u64 emc_zcal_interval_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_ZCAL_INTERVAL;
	u64 emc_pmacro_autocal_cfg_common_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_PMACRO_AUTOCAL_CFG_COMMON;
	u64 emc_pmacro_data_pad_tx_ctrl_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_PMACRO_DATA_PAD_TX_CTRL;
	u64 emc_pmacro_cmd_pad_tx_ctrl_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_PMACRO_CMD_PAD_TX_CTRL;
	u64 emc_pmacro_brick_ctrl_rfu1_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_PMACRO_BRICK_CTRL_RFU1;
	u64 emc_pmacro_common_pad_tx_ctrl_ab =
		(u64)IO_ADDRESS(TEGRA_EMC_BASE) + EMC_PMACRO_COMMON_PAD_TX_CTRL;
	u32 opt_war_200024907;
	u32 zq_wait_long;
	u32 zq_wait_short;

	u32 bg_regulator_switch_complete_wait_clks;
	u32 bg_regulator_mode_change;
	u32 enable_bglp_regulator;
	u32 enable_bg_regulator;

	u32 tRTM;
	u32 RP_war;
	u32 R2P_war;
	u32 TRPab_war;
	s32 nRTP;
	u32 deltaTWATM;
	u32 W2P_war;
	u32 tRPST;

	static u32 fsp_for_next_freq;

	emc_cc_dbg(INFO, "Running clock change.");
	ccfifo_index = 0;

	fake_timing = get_timing_from_freq(last_timing->rate);

	fsp_for_next_freq = !fsp_for_next_freq;

	dram_type = emc_readl(EMC_FBIO_CFG5) &
		EMC_FBIO_CFG5_DRAM_TYPE_MASK >> EMC_FBIO_CFG5_DRAM_TYPE_SHIFT;
	shared_zq_resistor = last_timing->burst_regs[EMC_ZCAL_WAIT_CNT_INDEX] &
		1 << 31; /* needs def */
	channel_mode = !!(last_timing->burst_regs[EMC_FBIO_CFG7_INDEX] &
			  1 << 2); /* needs def */
	opt_zcal_en_cc = (next_timing->burst_regs[EMC_ZCAL_INTERVAL_INDEX] &&
			  !last_timing->burst_regs[EMC_ZCAL_INTERVAL_INDEX]) ||
			  dram_type == DRAM_TYPE_LPDDR4;
	opt_dll_mode = (dram_type == DRAM_TYPE_DDR3) ?
		get_dll_state(next_timing) : DLL_OFF;
	is_lpddr3 = (dram_type == DRAM_TYPE_LPDDR2) &&
		next_timing->burst_regs[EMC_FBIO_CFG5_INDEX] &
		1 << 25; /* needs def */
	opt_war_200024907 = (dram_type == DRAM_TYPE_LPDDR4);
	opt_dvfs_mode = MAN_SR;
	dram_dev_num = (mc_readl(MC_EMEM_ADR_CFG) & 0x1) + 1;

	emc_cfg_reg = emc_readl(EMC_CFG);
	emc_auto_cal_config = emc_readl(EMC_AUTO_CAL_CONFIG);

	source_clock_period = 1000000000 / last_timing->rate;
	destination_clock_period = 1000000000 / next_timing->rate;

	tZQCAL_lpddr4_fc_adj = (source_clock_period > zqcal_before_cc_cutoff) ?
		tZQCAL_lpddr4 / destination_clock_period :
		(tZQCAL_lpddr4 - tFC_lpddr4) / destination_clock_period;

	emc_dbg_o = emc_readl(EMC_DBG);
	emc_pin_o = emc_readl(EMC_PIN);
	emc_cfg_pipe_clk_o = emc_readl(EMC_CFG_PIPE_CLK);
	emc_dbg = emc_dbg_o;

	emc_cfg = next_timing->burst_regs[EMC_CFG_INDEX];
	emc_cfg &= ~(EMC_CFG_DYN_SELF_REF | EMC_CFG_DRAM_ACPD |
		     EMC_CFG_DRAM_CLKSTOP_SR | EMC_CFG_DRAM_CLKSTOP_PD);
	emc_sel_dpd_ctrl = next_timing->emc_sel_dpd_ctrl;
	emc_sel_dpd_ctrl &= ~(EMC_SEL_DPD_CTRL_CLK_SEL_DPD_EN |
			      EMC_SEL_DPD_CTRL_CA_SEL_DPD_EN |
			      EMC_SEL_DPD_CTRL_RESET_SEL_DPD_EN |
			      EMC_SEL_DPD_CTRL_ODT_SEL_DPD_EN |
			      EMC_SEL_DPD_CTRL_DATA_SEL_DPD_EN);

	emc_cc_dbg(INFO, "Clock change version: %d\n",
		   DVFS_CLOCK_CHANGE_VERSION);
	emc_cc_dbg(INFO, "DRAM type = %d\n", dram_type);
	emc_cc_dbg(INFO, "DRAM dev #: %d\n", dram_dev_num);
	emc_cc_dbg(INFO, "Next EMC clksrc: 0x%08x\n", clksrc);
	emc_cc_dbg(INFO, "DLL clksrc:      0x%08x\n", next_timing->dll_clk_src);
	emc_cc_dbg(INFO, "last rate: %lu, next rate %lu\n", last_timing->rate,
		   next_timing->rate);
	emc_cc_dbg(INFO, "last period: %u, next period: %u\n",
		   source_clock_period, destination_clock_period);
	emc_cc_dbg(INFO, "  shared_zq_resistor: %d\n", !!shared_zq_resistor);
	emc_cc_dbg(INFO, "  channel_mode: %d\n", channel_mode);
	emc_cc_dbg(INFO, "  opt_dll_mode: %d\n", opt_dll_mode);

	/* Step 1:
	 *   Pre DVFS SW sequence.
	 */
	emc_cc_dbg(STEPS, "Step 1\n");
	emc_cc_dbg(SUB_STEPS, "Step 1.1: Bug 200024907 - Patch RP R2P");

	if (opt_war_200024907) {
		nRTP = 16;
		if (source_clock_period >= 1000000/1866) /* 535.91 ps */
			nRTP = 14;
		if (source_clock_period >= 1000000/1600) /* 625.00 ps */
			nRTP = 12;
		if (source_clock_period >= 1000000/1333) /* 750.19 ps */
			nRTP = 10;
		if (source_clock_period >= 1000000/1066) /* 938.09 ps */
			nRTP = 8;

		deltaTWATM = max_t(u32, div_o3(7500, source_clock_period), 8);

		/*
		 * Originally there was a + .5 in the tRPST calculation.
		 * However since we can't do FP in the kernel and the tRTM
		 * computation was in a floating point ceiling function, adding
		 * one to tRTP should be ok. There is no other source of non
		 * integer values, so the result was always going to be
		 * something for the form: f_ceil(N + .5) = N + 1;
		 */
		tRPST = ((last_timing->emc_mrw & 0x80) >> 7);
		tRTM = fake_timing->dram_timing_regs[RL] +
			div_o3(3600, source_clock_period) +
			max_t(u32, div_o3(7500, source_clock_period), 8) +
			tRPST + 1 + nRTP;

		emc_cc_dbg(INFO, "tRTM = %u, EMC_RP = %u\n", tRTM,
			   next_timing->burst_regs[EMC_RP_INDEX]);

		if (last_timing->burst_regs[EMC_RP_INDEX] < tRTM) {
			if (tRTM > (last_timing->burst_regs[EMC_R2P_INDEX] +
				    last_timing->burst_regs[EMC_RP_INDEX])) {
				R2P_war = tRTM -
					last_timing->burst_regs[EMC_RP_INDEX];
				RP_war = last_timing->burst_regs[EMC_RP_INDEX];
				TRPab_war =
				       last_timing->burst_regs[EMC_TRPAB_INDEX];
				if (R2P_war > 63) {
					RP_war = R2P_war +
						last_timing->burst_regs
						[EMC_RP_INDEX] - 63;
					if (TRPab_war < RP_war)
						TRPab_war = RP_war;
					R2P_war = 63;
				}
			} else {
				R2P_war = last_timing->
					burst_regs[EMC_R2P_INDEX];
				RP_war = last_timing->burst_regs[EMC_RP_INDEX];
				TRPab_war =
				       last_timing->burst_regs[EMC_TRPAB_INDEX];
			}

			if (RP_war < deltaTWATM) {
				W2P_war = last_timing->burst_regs[EMC_W2P_INDEX]
					+ deltaTWATM - RP_war;
				if (W2P_war > 63) {
					RP_war = RP_war + W2P_war - 63;
					if (TRPab_war < RP_war)
						TRPab_war = RP_war;
					W2P_war = 63;
				}
			} else {
				W2P_war =
					last_timing->burst_regs[EMC_W2P_INDEX];
			}

			if((last_timing->burst_regs[EMC_W2P_INDEX] ^ W2P_war) ||
			   (last_timing->burst_regs[EMC_R2P_INDEX] ^ R2P_war) ||
			   (last_timing->burst_regs[EMC_RP_INDEX] ^ RP_war) ||
			   (last_timing->burst_regs[EMC_TRPAB_INDEX] ^
			    TRPab_war)) {
				emc_writel(RP_war, EMC_RP);
				emc_writel(R2P_war, EMC_R2P);
				emc_writel(W2P_war, EMC_W2P);
				emc_writel(TRPab_war, EMC_TRPAB);
			}
			emc_timing_update(DUAL_CHANNEL);
		} else {
			emc_cc_dbg(INFO, "Skipped WAR for bug 200024907\n");
		}
	}

	emc_writel(EMC_INTSTATUS_CLKCHANGE_COMPLETE, EMC_INTSTATUS);
	emc_set_shadow_bypass(ACTIVE);
	emc_writel(emc_cfg, EMC_CFG);
	emc_writel(emc_sel_dpd_ctrl, EMC_SEL_DPD_CTRL);
	emc_writel(emc_cfg_pipe_clk_o | EMC_CFG_PIPE_CLK_CLK_ALWAYS_ON,
		   EMC_CFG_PIPE_CLK);
	emc_writel(next_timing->emc_fdpd_ctrl_cmd_no_ramp &
		   ~EMC_FDPD_CTRL_CMD_NO_RAMP_CMD_DPD_NO_RAMP_ENABLE,
		   EMC_FDPD_CTRL_CMD_NO_RAMP);

	bg_regulator_mode_change =
		((next_timing->burst_regs[EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
		  EMC_PMACRO_BG_BIAS_CTRL_0_BGLP_E_PWRD) ^
		 (last_timing->burst_regs[EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
		  EMC_PMACRO_BG_BIAS_CTRL_0_BGLP_E_PWRD)) ||
		((next_timing->burst_regs[EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
		  EMC_PMACRO_BG_BIAS_CTRL_0_BG_E_PWRD) ^
		 (last_timing->burst_regs[EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
		  EMC_PMACRO_BG_BIAS_CTRL_0_BG_E_PWRD));
	enable_bglp_regulator =
		(next_timing->burst_regs[EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
		 EMC_PMACRO_BG_BIAS_CTRL_0_BGLP_E_PWRD) == 0;
	enable_bg_regulator =
		(next_timing->burst_regs[EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
		 EMC_PMACRO_BG_BIAS_CTRL_0_BG_E_PWRD) == 0;

	if (bg_regulator_mode_change) {
		if (enable_bg_regulator)
			emc_writel(last_timing->burst_regs
				   [EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
				   ~EMC_PMACRO_BG_BIAS_CTRL_0_BG_E_PWRD,
				   EMC_PMACRO_BG_BIAS_CTRL_0);
		else
			emc_writel(last_timing->burst_regs
				   [EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
				   ~EMC_PMACRO_BG_BIAS_CTRL_0_BGLP_E_PWRD,
				   EMC_PMACRO_BG_BIAS_CTRL_0);

	}

	/* Check if we need to turn on VREF generator. */
	if ((((last_timing->burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX] &
	       EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_E_IVREF) == 0) &&
	     ((next_timing->burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX] &
	       EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_E_IVREF) == 1)) ||
	    (((last_timing->burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX] &
	       EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQS_E_IVREF) == 0) &&
	     ((next_timing->burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX] &
	       EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQS_E_IVREF) == 1))) {
		u32 pad_tx_ctrl =
		    next_timing->burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX];
		u32 last_pad_tx_ctrl =
		    last_timing->burst_regs[EMC_PMACRO_DATA_PAD_TX_CTRL_INDEX];

		next_dqs_e_ivref = pad_tx_ctrl &
			EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQS_E_IVREF;
		next_dq_e_ivref = pad_tx_ctrl &
			EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_E_IVREF;
		next_push = (last_pad_tx_ctrl &
			     ~EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_E_IVREF &
			     ~EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQS_E_IVREF) |
			next_dq_e_ivref | next_dqs_e_ivref;
		emc_writel(next_push, EMC_PMACRO_DATA_PAD_TX_CTRL);
		udelay(1);
	} else if (bg_regulator_mode_change) {
		udelay(1);
	}

	emc_set_shadow_bypass(ASSEMBLY);

	/* Step 2:
	 *   Prelock the DLL.
	 */
	emc_cc_dbg(STEPS, "Step 2\n");
	if (next_timing->burst_regs[EMC_CFG_DIG_DLL_INDEX] &
	    EMC_CFG_DIG_DLL_CFG_DLL_EN) {
		emc_cc_dbg(INFO, "Prelock enabled for target frequency.\n");
		dll_out = dll_prelock(next_timing, 0, clksrc);
		emc_cc_dbg(INFO, "DLL out: 0x%03x\n", dll_out);
		prelock_dll_en = 1;
	} else {
		emc_cc_dbg(INFO, "Disabling DLL for target frequency.\n");
		dll_disable(channel_mode);
	}

	/* Step 3:
	 *   Prepare autocal for the clock change.
	 */
	emc_cc_dbg(STEPS, "Step 3\n");
	emc_auto_cal_config = next_timing->emc_auto_cal_config;
	auto_cal_en = emc_auto_cal_config & EMC_AUTO_CAL_CONFIG_AUTO_CAL_ENABLE;
	emc_auto_cal_config &= ~EMC_AUTO_CAL_CONFIG_AUTO_CAL_START;
	emc_auto_cal_config |=  EMC_AUTO_CAL_CONFIG_AUTO_CAL_MEASURE_STALL;
	emc_auto_cal_config |=  EMC_AUTO_CAL_CONFIG_AUTO_CAL_UPDATE_STALL;
	emc_auto_cal_config |=  auto_cal_en;
	emc_writel(emc_auto_cal_config, EMC_AUTO_CAL_CONFIG);

	emc_set_shadow_bypass(ACTIVE);
	emc_writel(next_timing->emc_auto_cal_config2, EMC_AUTO_CAL_CONFIG2);
	emc_writel(next_timing->emc_auto_cal_config3, EMC_AUTO_CAL_CONFIG3);
	emc_writel(next_timing->emc_auto_cal_config4, EMC_AUTO_CAL_CONFIG4);
	emc_writel(next_timing->emc_auto_cal_config5, EMC_AUTO_CAL_CONFIG5);
	emc_writel(next_timing->emc_auto_cal_config6, EMC_AUTO_CAL_CONFIG6);
	emc_writel(next_timing->emc_auto_cal_config7, EMC_AUTO_CAL_CONFIG7);
	emc_writel(next_timing->emc_auto_cal_config8, EMC_AUTO_CAL_CONFIG8);
	emc_set_shadow_bypass(ASSEMBLY);

	emc_auto_cal_config |= (EMC_AUTO_CAL_CONFIG_AUTO_CAL_COMPUTE_START |
				auto_cal_en);
	emc_writel(emc_auto_cal_config, EMC_AUTO_CAL_CONFIG);

	/* Step 4:
	 *   Update EMC_CFG. (??)
	 */
	emc_cc_dbg(STEPS, "Step 4\n");
	if (source_clock_period > 50000 && dram_type == DRAM_TYPE_LPDDR4)
		ccfifo_writel(1, EMC_SELF_REF, 0);
	else
		emc_writel(next_timing->emc_cfg_2, EMC_CFG_2);

	/* Step 5:
	 *   Prepare reference variables for ZQCAL regs.
	 */
	emc_cc_dbg(STEPS, "Step 5\n");
	emc_zcal_interval = 0;
	emc_zcal_wait_cnt_old =
		last_timing->burst_regs[EMC_ZCAL_WAIT_CNT_INDEX];
	emc_zcal_wait_cnt_new =
		next_timing->burst_regs[EMC_ZCAL_WAIT_CNT_INDEX];
	emc_zcal_wait_cnt_old &= ~EMC_ZCAL_WAIT_CNT_ZCAL_WAIT_CNT_MASK;
	emc_zcal_wait_cnt_new &= ~EMC_ZCAL_WAIT_CNT_ZCAL_WAIT_CNT_MASK;

	if (dram_type == DRAM_TYPE_LPDDR4)
		zq_wait_long = max((u32)1,
				 div_o3(1000000, destination_clock_period));
	else if (dram_type == DRAM_TYPE_LPDDR2 || is_lpddr3)
		zq_wait_long = max(next_timing->min_mrs_wait,
				 div_o3(360000, destination_clock_period)) + 4;
	else if (dram_type == DRAM_TYPE_DDR3)
		zq_wait_long = max((u32)256,
				 div_o3(320000, destination_clock_period) + 2);
	else
		zq_wait_long = 0;

	if (dram_type == DRAM_TYPE_LPDDR2 || is_lpddr3)
		zq_wait_short = max(max(next_timing->min_mrs_wait, (u32)6),
				  div_o3(90000, destination_clock_period)) + 4;
	else if (dram_type == DRAM_TYPE_DDR3)
		zq_wait_short = max((u32)64,
				  div_o3(80000, destination_clock_period)) + 2;
	else
		zq_wait_short = 0;

	/* Step 6:
	 *   Training code - removed.
	 */
	emc_cc_dbg(STEPS, "Step 6\n");

	/* Step 7:
	 *   Program FSP reference registers and send MRWs to new FSPWR.
	 */
	emc_cc_dbg(STEPS, "Step 7\n");
	if (!fsp_for_next_freq) {
		mr13_flip_fspwr = (next_timing->emc_mrw3 & 0xffffff3f) | 0x80;
		mr13_flip_fspop = (next_timing->emc_mrw3 & 0xffffff3f) | 0x00;
	} else {
		mr13_flip_fspwr = (next_timing->emc_mrw3 & 0xffffff3f) | 0x40;
		mr13_flip_fspop = (next_timing->emc_mrw3 & 0xffffff3f) | 0xc0;
	}

	mr13_catr_enable = (mr13_flip_fspwr & 0xFFFFFFFE) | 0x01;
	if (dram_dev_num == TWO_RANK)
		mr13_catr_enable =
			(mr13_catr_enable & 0x3fffffff) | 0x80000000;

	if (dram_type == DRAM_TYPE_LPDDR4) {
		emc_writel(mr13_flip_fspwr, EMC_MRW3);
		emc_writel(next_timing->emc_mrw, EMC_MRW);
		emc_writel(next_timing->emc_mrw2, EMC_MRW2);
	}

	/* Step 8:
	 *   Program the shadow registers.
	 */
	emc_cc_dbg(STEPS, "Step 8\n");
	emc_cc_dbg(SUB_STEPS, "Writing burst_regs\n");
	for (i = 0; i < next_timing->burst_regs_num; i++) {
		u64 var;
		u32 wval;

		if (!burst_reg_off[i])
			continue;

		var = (u64)burst_reg_off[i];
		wval = next_timing->burst_regs[i];

		if (dram_type != DRAM_TYPE_LPDDR4 &&
		    (var == emc_mrw6_ab      || var == emc_mrw7_ab ||
		     var == emc_mrw8_ab      || var == emc_mrw9_ab ||
		     var == emc_mrw10_ch0_ab || var == emc_mrw10_ch1_ab ||
		     var == emc_mrw11_ch0_ab || var == emc_mrw11_ch1_ab ||
		     var == emc_mrw12_ch0_ab || var == emc_mrw12_ch1_ab ||
		     var == emc_mrw13_ch0_ab || var == emc_mrw13_ch1_ab ||
		     var == emc_mrw14_ab     || var == emc_mrw15_ab ||
		     var == emc_training_ctrl_ab))
			continue;

		/* Pain... And suffering. */
		if (var == emc_cfg_ab) {
			wval &= ~EMC_CFG_DRAM_ACPD;
			wval &= ~EMC_CFG_DYN_SELF_REF;
			if (dram_type == DRAM_TYPE_LPDDR4) {
				wval &= ~EMC_CFG_DRAM_CLKSTOP_SR;
				wval &= ~EMC_CFG_DRAM_CLKSTOP_PD;
			}
		} else if (var == emc_mrs_wait_cnt_ab &&
			   dram_type == DRAM_TYPE_LPDDR2 &&
			   opt_zcal_en_cc && !opt_cc_short_zcal &&
			   opt_short_zcal) {
			wval = (wval & ~(EMC_MRS_WAIT_CNT_SHORT_WAIT_MASK <<
					 EMC_MRS_WAIT_CNT_SHORT_WAIT_SHIFT)) |
			   ((zq_wait_long & EMC_MRS_WAIT_CNT_SHORT_WAIT_MASK) <<
			    EMC_MRS_WAIT_CNT_SHORT_WAIT_SHIFT);
		} else if (var == emc_zcal_wait_cnt_ab &&
			   dram_type == DRAM_TYPE_DDR3 && opt_zcal_en_cc &&
			   !opt_cc_short_zcal && opt_short_zcal) {
			wval = (wval & ~(EMC_ZCAL_WAIT_CNT_ZCAL_WAIT_CNT_MASK <<
				       EMC_ZCAL_WAIT_CNT_ZCAL_WAIT_CNT_SHIFT)) |
			    ((zq_wait_long &
			      EMC_ZCAL_WAIT_CNT_ZCAL_WAIT_CNT_MASK) <<
			      EMC_MRS_WAIT_CNT_SHORT_WAIT_SHIFT);
		} else if (var == emc_zcal_interval_ab && opt_zcal_en_cc) {
			wval = 0; /* EMC_ZCAL_INTERVAL reset value. */
		} else if (var == emc_pmacro_autocal_cfg_common_ab) {
			wval |= EMC_PMACRO_AUTOCAL_CFG_COMMON_E_CAL_BYPASS_DVFS;
		} else if (var == emc_pmacro_data_pad_tx_ctrl_ab) {
			wval &=
			     ~(EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSP_TX_E_DCC |
			       EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQSN_TX_E_DCC |
			       EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_DQ_TX_E_DCC |
			       EMC_PMACRO_DATA_PAD_TX_CTRL_DATA_CMD_TX_E_DCC);
		} else if (var == emc_pmacro_cmd_pad_tx_ctrl_ab) {
			wval |= EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_DRVFORCEON;
			wval &= ~(EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSP_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQSN_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_DQ_TX_E_DCC |
				  EMC_PMACRO_CMD_PAD_TX_CTRL_CMD_CMD_TX_E_DCC);
		} else if (var == emc_pmacro_brick_ctrl_rfu1_ab) {
			wval &= 0xf800f800;
		} else if (var == emc_pmacro_common_pad_tx_ctrl_ab) {
			wval &= 0xfffffff0;
		}

		emc_cc_dbg(REG_LISTS, "(%u) 0x%08x => 0x%p\n",
			   i, wval, (void *)var);
		__raw_writel(wval, (void __iomem *)var);
	}

	/* Per channel burst registers. */
	emc_cc_dbg(SUB_STEPS, "Writing burst_regs_per_ch\n");
	for (i = 0; i < next_timing->burst_regs_per_ch_num; i++) {
		if (!burst_perch_reg_off[i])
			continue;

		if (dram_type != DRAM_TYPE_LPDDR4 &&
		    ((u64)burst_perch_reg_off[i] == emc_mrw6_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw7_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw8_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw9_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw10_ch0_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw10_ch1_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw11_ch0_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw11_ch1_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw12_ch0_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw12_ch1_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw13_ch0_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw13_ch1_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw14_ab ||
		     (u64)burst_perch_reg_off[i] == emc_mrw15_ab))
			continue;

		/* Filter out second channel if not in DUAL_CHANNEL mode. */
		if (channel_mode != DUAL_CHANNEL &&
		    (u64)burst_perch_reg_off[i] >=
		    (u64)IO_ADDRESS(TEGRA_EMC1_BASE))
			continue;

		emc_cc_dbg(REG_LISTS, "(%u) 0x%08x => 0x%p\n",
			   i, next_timing->burst_regs_per_ch[i],
			   burst_perch_reg_off[i]);
		__raw_writel(next_timing->burst_regs_per_ch[i],
			     burst_perch_reg_off[i]);
	}

	/* Vref regs. */
	emc_cc_dbg(SUB_STEPS, "Writing vref_regs\n");
	for (i = 0; i < next_timing->vref_regs_num; i++) {
		if (!vref_reg_off[i])
			continue;

		if (channel_mode != DUAL_CHANNEL &&
		    (u64)vref_reg_off[i] >= (u64)IO_ADDRESS(TEGRA_EMC1_BASE))
			continue;

		emc_cc_dbg(REG_LISTS, "(%u) 0x%08x => 0x%p\n",
			   i, next_timing->vref_regs[i], vref_reg_off[i]);
		__raw_writel(next_timing->vref_regs[i], vref_reg_off[i]);
	}

	/* Trimmers. */
	emc_cc_dbg(SUB_STEPS, "Writing trim_regs\n");
	for (i = 0; i < next_timing->trim_regs_num; i++) {
		if (!trim_reg_off[i])
			continue;

		emc_cc_dbg(REG_LISTS, "(%u) 0x%08x => 0x%p\n",
			   i, next_timing->trim_regs[i],
			   trim_reg_off[i]);
		__raw_writel(next_timing->trim_regs[i], trim_reg_off[i]);
	}

	/* Per channel trimmers. */
	emc_cc_dbg(SUB_STEPS, "Writing trim_regs_per_ch\n");
	for (i = 0; i < next_timing->trim_regs_per_ch_num; i++) {
		if (!trim_perch_reg_off[i])
			continue;

		if (channel_mode != DUAL_CHANNEL &&
		    (u64)vref_reg_off[i] >=
		    (u64)IO_ADDRESS(TEGRA_EMC1_BASE))
			continue;

		emc_cc_dbg(REG_LISTS, "(%u) 0x%08x => 0x%p\n",
			   i, next_timing->trim_regs_per_ch[i],
			   trim_perch_reg_off[i]);
		__raw_writel(next_timing->trim_regs_per_ch[i],
			     trim_perch_reg_off[i]);
	}

	emc_cc_dbg(SUB_STEPS, "Writing burst_mc_regs\n");
	for (i = 0; i < next_timing->burst_mc_regs_num; i++) {
		emc_cc_dbg(REG_LISTS, "(%u) 0x%08x => 0x%p\n",
			   i, next_timing->burst_mc_regs[i],
			   burst_mc_reg_off[i]);
		__raw_writel(next_timing->burst_mc_regs[i],
			     burst_mc_reg_off[i]);
	}

	/* Registers to be programmed on the faster clock. */
	if (next_timing->rate < last_timing->rate) {
		emc_cc_dbg(SUB_STEPS, "Writing la_scale_regs\n");
		for (i = 0; i < next_timing->la_scale_regs_num; i++) {
			emc_cc_dbg(REG_LISTS, "(%u) 0x%08x => 0x%p\n",
				   i, next_timing->la_scale_regs[i],
				   la_scale_off_regs[i]);
			__raw_writel(next_timing->la_scale_regs[i],
				     la_scale_off_regs[i]);
		}
	}

	/* Flush all the burst register writes. */
	wmb();

	/* Step 9:
	 *   LPDDR4 section A.
	 */
	emc_cc_dbg(STEPS, "Step 9\n");
	if (dram_type == DRAM_TYPE_LPDDR4) {
		emc_writel(emc_zcal_interval, EMC_ZCAL_INTERVAL);
		emc_writel(emc_zcal_wait_cnt_new, EMC_ZCAL_WAIT_CNT);

		emc_dbg |= (EMC_DBG_WRITE_MUX_ACTIVE |
			    EMC_DBG_WRITE_ACTIVE_ONLY);

		emc_writel(emc_dbg, EMC_DBG);
		emc_writel(emc_zcal_interval, EMC_ZCAL_INTERVAL);
		emc_writel(emc_dbg_o, EMC_DBG);
	}

	/* Step 10:
	 *   LPDDR4 and DDR3 common section.
	 */
	emc_cc_dbg(STEPS, "Step 10\n");
	if (opt_dvfs_mode == MAN_SR || dram_type == DRAM_TYPE_LPDDR4) {
		if (dram_type == DRAM_TYPE_LPDDR4)
			ccfifo_writel(0x101, EMC_SELF_REF, 0);
		else
			ccfifo_writel(0x1, EMC_SELF_REF, 0);

		if (dram_type == DRAM_TYPE_LPDDR4 &&
		    source_clock_period <= zqcal_before_cc_cutoff) {
			ccfifo_writel(mr13_flip_fspwr ^ 0x40, EMC_MRW3, 0);
			ccfifo_writel((next_timing->burst_regs[EMC_MRW6_INDEX] &
				       0xFFFF3F3F) |
				      (last_timing->burst_regs[EMC_MRW6_INDEX] &
				       0x0000C0C0), EMC_MRW6, 0);
			ccfifo_writel(
				(next_timing->burst_regs[EMC_MRW14_INDEX] &
				 0xFFFF0707) |
				(last_timing->burst_regs[EMC_MRW14_INDEX] &
				 0x00003838), EMC_MRW14, 0);

			if (dram_dev_num == TWO_RANK) {
				ccfifo_writel(
				      (next_timing->burst_regs[EMC_MRW7_INDEX] &
				       0xFFFF3F3F) |
				      (last_timing->burst_regs[EMC_MRW7_INDEX] &
				       0x0000C0C0), EMC_MRW7, 0);
				ccfifo_writel(
				     (next_timing->burst_regs[EMC_MRW15_INDEX] &
				      0xFFFF0707) |
				     (last_timing->burst_regs[EMC_MRW15_INDEX] &
				      0x00003838), EMC_MRW15, 0);
			}
			if (opt_zcal_en_cc) {
				if (dram_dev_num == ONE_RANK)
					ccfifo_writel(
						2 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
						EMC_ZQ_CAL_ZQ_CAL_CMD,
						EMC_ZQ_CAL, 0);
				else if (shared_zq_resistor)
					ccfifo_writel(
						2 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
						EMC_ZQ_CAL_ZQ_CAL_CMD,
						EMC_ZQ_CAL, 0);
				else
					ccfifo_writel(EMC_ZQ_CAL_ZQ_CAL_CMD,
						     EMC_ZQ_CAL, 0);
			}
		}
	}

	emc_dbg = emc_dbg_o;
	if (dram_type == DRAM_TYPE_LPDDR4) {
		ccfifo_writel(mr13_flip_fspop | 0x8, EMC_MRW3,
			      (1000 * fake_timing->dram_timing_regs[T_RP]) /
			      source_clock_period);
		ccfifo_writel(0, 0, tFC_lpddr4 / source_clock_period);
	}

	if (dram_type == DRAM_TYPE_LPDDR4 || opt_dvfs_mode != MAN_SR) {
		u32 t = 30 + (cya_allow_ref_cc ?
			(4000 * fake_timing->dram_timing_regs[T_RFC]) +
			((1000 * fake_timing->dram_timing_regs[T_RP]) /
			 source_clock_period) : 0);

		ccfifo_writel(emc_pin_o & ~(EMC_PIN_PIN_CKE_PER_DEV |
					    EMC_PIN_PIN_CKEB | EMC_PIN_PIN_CKE),
			      EMC_PIN, t);
	}

	ref_delay_mult = 1;
	ref_b4_sref_en = 0;
	cya_issue_pc_ref = 0;

	ref_delay_mult += ref_b4_sref_en   ? 1 : 0;
	ref_delay_mult += cya_allow_ref_cc ? 1 : 0;
	ref_delay_mult += cya_issue_pc_ref ? 1 : 0;
	ref_delay = ref_delay_mult *
		((1000 * fake_timing->dram_timing_regs[T_RP]
		  / source_clock_period) +
		 (1000 * fake_timing->dram_timing_regs[T_RFC] /
		  source_clock_period)) + 20;

	/* Step 11:
	 *   Ramp down.
	 */
	emc_cc_dbg(STEPS, "Step 11\n");
        ccfifo_writel(0x0, EMC_CFG_SYNC,
		      dram_type == DRAM_TYPE_LPDDR4 ? 0 : ref_delay);

	emc_dbg_active = emc_dbg | (EMC_DBG_WRITE_MUX_ACTIVE | /* Redundant. */
				    EMC_DBG_WRITE_ACTIVE_ONLY);
	ccfifo_writel(emc_dbg_active, EMC_DBG, 0);

	/* Todo: implement do_dvfs_power_ramp_down */
	ramp_down_wait = do_dvfs_power_ramp_down(source_clock_period, 0,
						 last_timing, next_timing);

	/* Step 12:
	 *   And finally - trigger the clock change.
	 */
	emc_cc_dbg(STEPS, "Step 12\n");
	ccfifo_writel(1, EMC_STALL_THEN_EXE_AFTER_CLKCHANGE, 0);
	emc_dbg_active &= ~EMC_DBG_WRITE_ACTIVE_ONLY;
	ccfifo_writel(emc_dbg_active, EMC_DBG, 0);

	/* Step 13:
	 *   Ramp up.
	 */
	/* Todo: implement do_dvfs_power_ramp_up(). */
	emc_cc_dbg(STEPS, "Step 13\n");
	ramp_up_wait = do_dvfs_power_ramp_up(destination_clock_period, 0,
					     last_timing, next_timing);
	ccfifo_writel(emc_dbg, EMC_DBG, 0);

	/* Step 14:
	 *   Bringup CKE pins.
	 */
	emc_cc_dbg(STEPS, "Step 14\n");
	if (dram_type == DRAM_TYPE_LPDDR4) {
		u32 r = emc_pin_o | EMC_PIN_PIN_CKE;
		if (dram_dev_num == TWO_RANK)
			ccfifo_writel(r | EMC_PIN_PIN_CKEB |
				      EMC_PIN_PIN_CKE_PER_DEV, EMC_PIN,
				      0);
		else
			ccfifo_writel(r & ~(EMC_PIN_PIN_CKEB |
					    EMC_PIN_PIN_CKE_PER_DEV),
				      EMC_PIN, 0);
	}

	/* Step 15: (two step 15s ??)
	 *   Calculate zqlatch wait time; has dependency on ramping times.
	 */
	emc_cc_dbg(STEPS, "Step 15\n");

	if (source_clock_period <= zqcal_before_cc_cutoff)
		zq_latch_dvfs_wait_time =
			(tZQCAL_lpddr4_fc_adj - (ramp_up_wait + ramp_down_wait))
			/ destination_clock_period;
	else
		zq_latch_dvfs_wait_time = tZQCAL_lpddr4_fc_adj -
			div_o3(1000 * next_timing->dram_timing_regs[T_PDEX],
			       destination_clock_period);

	emc_cc_dbg(INFO, "tZQCAL_lpddr4_fc_adj = %u\n", tZQCAL_lpddr4_fc_adj);
	emc_cc_dbg(INFO, "destination_clock_period = %u\n",
		   destination_clock_period);
	emc_cc_dbg(INFO, "next_timing->dram_timing_regs[T_PDEX] = %u\n",
		   next_timing->dram_timing_regs[T_PDEX]);
	emc_cc_dbg(INFO, "zq_latch_dvfs_wait_time = %d\n",
		   max_t(s32, 0, zq_latch_dvfs_wait_time));

	if (dram_type == DRAM_TYPE_LPDDR4 && opt_zcal_en_cc) {
		if (dram_dev_num == ONE_RANK) {
			if (source_clock_period > zqcal_before_cc_cutoff)
				ccfifo_writel(2 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
				   EMC_ZQ_CAL_ZQ_CAL_CMD, EMC_ZQ_CAL,
				   div_o3(1000 *
					  next_timing->dram_timing_regs[T_PDEX],
					  destination_clock_period));
			ccfifo_writel((mr13_flip_fspop & 0xFFFFFFF7) |
				   0x0C000000, EMC_MRW3,
				   div_o3(1000 *
					  next_timing->dram_timing_regs[T_PDEX],
					  destination_clock_period));
			ccfifo_writel(EMC_SELF_REF_ACTIVE_SELF_REF,
				      EMC_SELF_REF, 0);
			ccfifo_writel(0, EMC_REF, 0);
			ccfifo_writel(2 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
				      EMC_ZQ_CAL_ZQ_LATCH_CMD,
				      EMC_ZQ_CAL,
				      max_t(s32, 0, zq_latch_dvfs_wait_time));
		} else if (shared_zq_resistor) {
			if (source_clock_period > zqcal_before_cc_cutoff)
				ccfifo_writel(2 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
				   EMC_ZQ_CAL_ZQ_CAL_CMD, EMC_ZQ_CAL,
				   div_o3(1000 *
					  next_timing->dram_timing_regs[T_PDEX],
					  destination_clock_period));

			ccfifo_writel(2 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
				  EMC_ZQ_CAL_ZQ_LATCH_CMD, EMC_ZQ_CAL,
				  max_t(s32, 0, zq_latch_dvfs_wait_time) +
				  div_o3(1000 *
					 next_timing->dram_timing_regs[T_PDEX],
					 destination_clock_period));
			ccfifo_writel(1 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
				      EMC_ZQ_CAL_ZQ_LATCH_CMD,
				      EMC_ZQ_CAL, 0);

			ccfifo_writel((mr13_flip_fspop & 0xfffffff7) |
				      0x0c000000, EMC_MRW3, 0);
			ccfifo_writel(EMC_SELF_REF_ACTIVE_SELF_REF,
				      EMC_SELF_REF, 0);
			ccfifo_writel(0, EMC_REF, 0);

			ccfifo_writel(1 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
				      EMC_ZQ_CAL_ZQ_LATCH_CMD, EMC_ZQ_CAL,
				      tZQCAL_lpddr4 / destination_clock_period);
		} else {
			if (source_clock_period > zqcal_before_cc_cutoff) {
				ccfifo_writel(EMC_ZQ_CAL_ZQ_CAL_CMD, EMC_ZQ_CAL,
				   div_o3(1000 *
					  next_timing->dram_timing_regs[T_PDEX],
					  destination_clock_period));
			}

			ccfifo_writel((mr13_flip_fspop & 0xfffffff7) |
				   0x0c000000, EMC_MRW3,
				   div_o3(1000 *
					  next_timing->dram_timing_regs[T_PDEX],
					  destination_clock_period));
			ccfifo_writel(EMC_SELF_REF_ACTIVE_SELF_REF,
				      EMC_SELF_REF, 0);
			ccfifo_writel(0, EMC_REF, 0);

			ccfifo_writel(EMC_ZQ_CAL_ZQ_LATCH_CMD, EMC_ZQ_CAL,
				      max_t(s32, 0, zq_latch_dvfs_wait_time));
		}
	}

	/* WAR: delay for zqlatch */
	ccfifo_writel(0, 0, 10);

	/* Step 16:
	 *   LPDDR4 Conditional Training Kickoff. Removed.
	 */

	/* Step 17:
	 *   MANSR exit self refresh.
	 */
	emc_cc_dbg(STEPS, "Step 17\n");
	if (opt_dvfs_mode == MAN_SR && dram_type != DRAM_TYPE_LPDDR4)
		ccfifo_writel(0, EMC_SELF_REF, 0);

	/* Step 18:
	 *   Send MRWs to LPDDR3/DDR3.
	 */
	emc_cc_dbg(STEPS, "Step 18\n");
	if (dram_type == DRAM_TYPE_LPDDR2) {
		ccfifo_writel(next_timing->emc_mrw2, EMC_MRW2, 0);
		ccfifo_writel(next_timing->emc_mrw,  EMC_MRW,  0);
		if (is_lpddr3)
			ccfifo_writel(next_timing->emc_mrw4, EMC_MRW4, 0);
	} else if (dram_type == DRAM_TYPE_DDR3) {
		if (opt_dll_mode == DLL_ON)
			ccfifo_writel(next_timing->emc_emrs &
				      ~EMC_EMRS_USE_EMRS_LONG_CNT, EMC_EMRS, 0);
		ccfifo_writel(next_timing->emc_emrs2 &
			      ~EMC_EMRS2_USE_EMRS2_LONG_CNT, EMC_EMRS2, 0);
		ccfifo_writel(next_timing->emc_mrs |
			      EMC_EMRS_USE_EMRS_LONG_CNT, EMC_MRS, 0);
	}

	/* Step 19:
	 *   ZQCAL for LPDDR3/DDR3
	 */
	emc_cc_dbg(STEPS, "Step 19\n");
	if (opt_zcal_en_cc) {
		if (dram_type == DRAM_TYPE_LPDDR2) {
			u32 r;

			zq_op = opt_cc_short_zcal  ? 0x56 : 0xAB;
			zcal_wait_time_ps = opt_cc_short_zcal  ? 90000 : 360000;
			zcal_wait_time_clocks = div_o3(zcal_wait_time_ps,
						    destination_clock_period);
			r = zcal_wait_time_clocks <<
				EMC_MRS_WAIT_CNT2_MRS_EXT2_WAIT_CNT_SHIFT |
				zcal_wait_time_clocks <<
				EMC_MRS_WAIT_CNT2_MRS_EXT1_WAIT_CNT_SHIFT;
			ccfifo_writel(r, EMC_MRS_WAIT_CNT2, 0);
			ccfifo_writel(2 << EMC_MRW_MRW_DEV_SELECTN_SHIFT |
				      EMC_MRW_USE_MRW_EXT_CNT |
				      10 << EMC_MRW_MRW_MA_SHIFT |
				      zq_op << EMC_MRW_MRW_OP_SHIFT,
				      EMC_MRW, 0);
			if (dram_dev_num == TWO_RANK) {
				r = 1 << EMC_MRW_MRW_DEV_SELECTN_SHIFT |
					EMC_MRW_USE_MRW_EXT_CNT |
					10 << EMC_MRW_MRW_MA_SHIFT |
					zq_op << EMC_MRW_MRW_OP_SHIFT;
				ccfifo_writel(r, EMC_MRW, 0);
			}
		} else if (dram_type == DRAM_TYPE_DDR3) {
			zq_op = opt_cc_short_zcal ? 0 : EMC_ZQ_CAL_LONG;
			ccfifo_writel(zq_op | 2 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
				      EMC_ZQ_CAL_ZQ_CAL_CMD, EMC_ZQ_CAL, 0);
			if (dram_dev_num == TWO_RANK)
				ccfifo_writel(zq_op |
					      1 << EMC_ZQ_CAL_DEV_SEL_SHIFT |
					      EMC_ZQ_CAL_ZQ_CAL_CMD,
					      EMC_ZQ_CAL, 0);
		}
	}

	if (bg_regulator_mode_change) {
		emc_set_shadow_bypass(ACTIVE);
		bg_regulator_switch_complete_wait_clks =
			ramp_up_wait > 1250000 ? 0 :
			(1250000 - ramp_up_wait) / destination_clock_period;
		ccfifo_writel(next_timing->burst_regs
			      [EMC_PMACRO_BG_BIAS_CTRL_0_INDEX],
			      EMC_PMACRO_BG_BIAS_CTRL_0,
			      bg_regulator_switch_complete_wait_clks);
		emc_set_shadow_bypass(ASSEMBLY);
	}

	/* Step 20:
	 *   Issue ref and optional QRST.
	 */
	emc_cc_dbg(STEPS, "Step 20\n");
	if (dram_type != DRAM_TYPE_LPDDR4)
		ccfifo_writel(0, EMC_REF, 0);

	if (opt_do_sw_qrst) {
		ccfifo_writel(1, EMC_ISSUE_QRST, 0);
		ccfifo_writel(0, EMC_ISSUE_QRST, 2);
	}

	/* Step 21:
	 *   Restore ZCAL and ZCAL interval.
	 */
	emc_cc_dbg(STEPS, "Step 21\n");
        if (save_restore_clkstop_pd || opt_zcal_en_cc) {
		ccfifo_writel(emc_dbg_o | EMC_DBG_WRITE_MUX_ACTIVE, EMC_DBG, 0);
		if (opt_zcal_en_cc && dram_type != DRAM_TYPE_LPDDR4)
			ccfifo_writel(next_timing->
				      burst_regs[EMC_ZCAL_INTERVAL_INDEX],
				      EMC_ZCAL_INTERVAL, 0);

		if (save_restore_clkstop_pd)
			ccfifo_writel(next_timing->burst_regs[EMC_CFG_INDEX] &
				      ~EMC_CFG_DYN_SELF_REF, EMC_CFG, 0);
		ccfifo_writel(emc_dbg_o, EMC_DBG, 0);
	}

	/* Step 22:
	 *   Restore EMC_CFG_PIPE_CLK.
	 */
	emc_cc_dbg(STEPS, "Step 22\n");
	ccfifo_writel(emc_cfg_pipe_clk_o, EMC_CFG_PIPE_CLK, 0);

	if (bg_regulator_mode_change) {
		if (enable_bg_regulator)
			emc_writel(next_timing->burst_regs
				   [EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
				   ~EMC_PMACRO_BG_BIAS_CTRL_0_BGLP_E_PWRD,
				   EMC_PMACRO_BG_BIAS_CTRL_0);
		else
			emc_writel(next_timing->burst_regs
				   [EMC_PMACRO_BG_BIAS_CTRL_0_INDEX] &
				   ~EMC_PMACRO_BG_BIAS_CTRL_0_BG_E_PWRD,
				   EMC_PMACRO_BG_BIAS_CTRL_0);
	}

	/* Step 23:
	 */
	emc_cc_dbg(STEPS, "Step 23\n");

	/* Fix: rename tmp to something meaningful. */
	tmp = emc_readl(EMC_CFG_DIG_DLL);
	tmp |= EMC_CFG_DIG_DLL_CFG_DLL_STALL_ALL_TRAFFIC;
	tmp &= ~EMC_CFG_DIG_DLL_CFG_DLL_STALL_RW_UNTIL_LOCK;
	tmp &= ~EMC_CFG_DIG_DLL_CFG_DLL_STALL_ALL_UNTIL_LOCK;
	tmp = (tmp & ~EMC_CFG_DIG_DLL_CFG_DLL_MODE_MASK) |
		(2 << EMC_CFG_DIG_DLL_CFG_DLL_MODE_SHIFT);
	emc_writel(tmp, EMC_CFG_DIG_DLL);

	/* Clock change. Woot. BUG()s out if something fails. */
	do_clock_change(clksrc);

	/* Step 24:
	 *   Save training results. Removed.
	 */

	/* Step 25:
	 *   Program MC updown registers.
	 */
	emc_cc_dbg(STEPS, "Step 25\n");

	if (next_timing->rate > last_timing->rate) {
		for (i = 0; i < next_timing->la_scale_regs_num; i++)
			__raw_writel(next_timing->la_scale_regs[i],
				     la_scale_off_regs[i]);
		emc_timing_update(0);
	}

	/* Step 26:
	 *   Restore ZCAL registers.
	 */
	emc_cc_dbg(STEPS, "Step 26\n");
	if (dram_type == DRAM_TYPE_LPDDR4) {
		emc_set_shadow_bypass(ACTIVE);
		emc_writel(next_timing->burst_regs[EMC_ZCAL_WAIT_CNT_INDEX],
			   EMC_ZCAL_WAIT_CNT);
		emc_writel(next_timing->burst_regs[EMC_ZCAL_INTERVAL_INDEX],
			   EMC_ZCAL_INTERVAL);
		emc_set_shadow_bypass(ASSEMBLY);
	}

	if (dram_type != DRAM_TYPE_LPDDR4 &&
	    opt_zcal_en_cc && !opt_short_zcal && opt_cc_short_zcal) {
		udelay(2);

		emc_set_shadow_bypass(ACTIVE);
		if (dram_type == DRAM_TYPE_LPDDR2)
			emc_writel(next_timing->
				  burst_regs[EMC_MRS_WAIT_CNT_INDEX],
				  EMC_MRS_WAIT_CNT);
		else if (dram_type == DRAM_TYPE_DDR3)
			emc_writel(next_timing->
				   burst_regs[EMC_ZCAL_WAIT_CNT_INDEX],
				   EMC_ZCAL_WAIT_CNT);
		emc_set_shadow_bypass(ASSEMBLY);
	}

	/* Step 27:
	 *   Restore EMC_CFG, FDPD registers.
	 */
	emc_cc_dbg(STEPS, "Step 27\n");
	emc_set_shadow_bypass(ACTIVE);
	emc_writel(next_timing->burst_regs[EMC_CFG_INDEX], EMC_CFG);
	emc_set_shadow_bypass(ASSEMBLY);
	emc_writel(next_timing->emc_fdpd_ctrl_cmd_no_ramp,
		   EMC_FDPD_CTRL_CMD_NO_RAMP);
	emc_writel(next_timing->emc_sel_dpd_ctrl, EMC_SEL_DPD_CTRL);

	/* Step 28:
	 *   Training recover. Removed.
	 */
	emc_cc_dbg(STEPS, "Step 28\n");

	emc_set_shadow_bypass(ACTIVE);
	emc_writel(next_timing->burst_regs[EMC_PMACRO_AUTOCAL_CFG_COMMON_INDEX],
		   EMC_PMACRO_AUTOCAL_CFG_COMMON);
	emc_set_shadow_bypass(ASSEMBLY);

	/* Step 29:
	 *   Power fix WAR.
	 */
	emc_cc_dbg(STEPS, "Step 29\n");
	emc_writel(EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE0 |
		   EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE1 |
		   EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE2 |
		   EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE3 |
		   EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE4 |
		   EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE5 |
		   EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE6 |
		   EMC_PMACRO_CFG_PM_GLOBAL_0_DISABLE_CFG_BYTE7,
		   EMC_PMACRO_CFG_PM_GLOBAL_0);
	emc_writel(EMC_PMACRO_TRAINING_CTRL_0_CH0_TRAINING_E_WRPTR,
		   EMC_PMACRO_TRAINING_CTRL_0);
	emc_writel(EMC_PMACRO_TRAINING_CTRL_1_CH1_TRAINING_E_WRPTR,
		   EMC_PMACRO_TRAINING_CTRL_1);
	emc_writel(0, EMC_PMACRO_CFG_PM_GLOBAL_0);

	/* Step 30:
	 *   Re-enable autocal.
	 */
	emc_cc_dbg(STEPS, "Step 30\n");
	emc_auto_cal_config = next_timing->emc_auto_cal_config;
	emc_writel(emc_auto_cal_config, EMC_AUTO_CAL_CONFIG);

	/* Step 31:
	 *   Restore FSP to account for switch back. Only needed in training.
	 */
	emc_cc_dbg(STEPS, "Step 31\n");

	/* Done! Yay. */
}

static inline void emc_get_timing(struct tegra21_emc_table *timing)
{
	int i;

	/* Burst updates depends on previous state; burst_up_down are
	 * stateless. */
	for (i = 0; i < timing->burst_regs_num; i++) {
		if (burst_reg_off[i])
			timing->burst_regs[i] = __raw_readl(burst_reg_off[i]);
		else
			timing->burst_regs[i] = 0;
	}

	for (i = 0; i < timing->burst_regs_per_ch_num; i++)
		timing->burst_regs_per_ch[i] =
			__raw_readl(burst_perch_reg_off[i]);

	for (i = 0; i < timing->trim_regs_num; i++)
		timing->trim_regs[i] = __raw_readl(trim_reg_off[i]);

	for (i = 0; i < timing->trim_regs_per_ch_num; i++)
		timing->trim_regs_per_ch[i] =
			__raw_readl(trim_perch_reg_off[i]);

	for (i = 0; i < timing->vref_regs_num; i++)
		timing->vref_regs[i] = __raw_readl(vref_reg_off[i]);

	for (i = 0; i < timing->burst_mc_regs_num; i++)
		timing->burst_mc_regs[i] = __raw_readl(burst_mc_reg_off[i]);

	for (i = 0; i < timing->la_scale_regs_num; i++)
		timing->la_scale_regs[i] = __raw_readl(la_scale_off_regs[i]);

	/* TODO: fill in necessary table registers. */

	timing->rate = clk_get_rate_locked(emc) / 1000;
}

/* FIXME: expose latency interface */
u32 tegra21_get_dvfs_clk_change_latency_nsec(unsigned long emc_freq_khz)
{
	int i;

	if (!tegra_emc_table)
		goto default_val;

	if (emc_freq_khz > tegra_emc_table[tegra_emc_table_size - 1].rate) {
		i = tegra_emc_table_size - 1;
		if (tegra_emc_table[i].clock_change_latency != 0)
			return tegra_emc_table[i].clock_change_latency;
		else
			goto default_val;
	}

	for (i = get_start_idx(emc_freq_khz); i < tegra_emc_table_size; i++) {
		if (tegra_emc_table[i].rate == emc_freq_khz)
			break;

		if (tegra_emc_table[i].rate > emc_freq_khz) {
			/* emc_freq_khz was not found in the emc table. Use the
			   DVFS latency value of the EMC frequency just below
			   emc_freq_khz. */
			i--;
			break;
		}
	}

	if (tegra_emc_table[i].clock_change_latency != 0)
		return tegra_emc_table[i].clock_change_latency;

default_val:
	/* The DVFS clock change latency value couldn't be found. Use
	   a default value. */
	WARN_ONCE(1, "%s: Couldn't find DVFS clock change latency "
			"value - using default value\n",
		__func__);
	return 2000;
}

static const struct tegra21_emc_table *emc_get_table(
	unsigned long over_temp_state)
{
	if ((over_temp_state == DRAM_OVER_TEMP_THROTTLE) &&
	    (tegra_emc_table_derated != NULL))
		return tegra_emc_table_derated;
	else
		return tegra_emc_table;
}

/* The EMC registers have shadow registers. When the EMC clock is updated
 * in the clock controller, the shadow registers are copied to the active
 * registers, allowing glitchless memory bus frequency changes.
 * This function updates the shadow registers for a new clock frequency,
 * and relies on the clock lock on the emc clock to avoid races between
 * multiple frequency changes. In addition access lock prevents concurrent
 * access to EMC registers from reading MRR registers */
int tegra_emc_set_rate_on_parent(unsigned long rate, struct clk *p)
{
	int i;
	u32 clk_setting;
	const struct tegra21_emc_table *last_timing;
	const struct tegra21_emc_table *current_table;
	unsigned long flags;
	s64 last_change_delay;
	struct emc_sel *sel;

	if (!tegra_emc_table)
		return -EINVAL;

	/* Table entries specify rate in kHz */
	rate = rate / 1000;

	i = get_start_idx(rate);
	for (; i < tegra_emc_table_size; i++) {
		if (tegra_emc_clk_sel[i].input == NULL)
			continue;	/* invalid entry */

		if (tegra_emc_table[i].rate == rate)
			break;
	}

	if (i >= tegra_emc_table_size)
		return -EINVAL;

	if (!emc_timing) {
		/* can not assume that boot timing matches dfs table even
		   if boot frequency matches one of the table nodes */
		emc_get_timing(&start_timing);
		last_timing = &start_timing;
	} else
		last_timing = emc_timing;

	/* Select settings of matching pll_m(b) */
	sel = &tegra_emc_clk_sel[i];
	clk_setting = (p == sel->input) ?
		sel->value : tegra_emc_clk_sel_b[i].value;

	if (!timekeeping_suspended) {
		last_change_delay = ktime_us_delta(ktime_get(), clkchange_time);
		if ((last_change_delay >= 0) &&
		    (last_change_delay < clkchange_delay))
			udelay(clkchange_delay - (int)last_change_delay);
	}

	spin_lock_irqsave(&emc_access_lock, flags);
	/* Pick EMC table based on the status of the over temp state flag */
	current_table = emc_get_table(dram_over_temp_state);
	emc_set_clock(&current_table[i], last_timing, 0, clk_setting);
	clkchange_time = timekeeping_suspended ? clkchange_time : ktime_get();
	emc_timing = &current_table[i];
	tegra_mc_divider_update(emc);
	spin_unlock_irqrestore(&emc_access_lock, flags);

	emc_last_stats_update(i);

	pr_debug("%s: rate %lu setting 0x%x\n", __func__, rate, clk_setting);

	return 0;
}

long tegra_emc_round_rate_updown(unsigned long rate, bool up)
{
	int i;
	unsigned long table_rate;

	if (!tegra_emc_table)
		return clk_get_rate_locked(emc); /* no table - no rate change */

	if (!emc_enable)
		return -EINVAL;

	pr_debug("%s: %lu\n", __func__, rate);

	/* Table entries specify rate in kHz */
	rate = rate / 1000;

	i = get_start_idx(rate);
	for (; i < tegra_emc_table_size; i++) {
		if (tegra_emc_clk_sel[i].input == NULL)
			continue;	/* invalid entry */

		table_rate = tegra_emc_table[i].rate;
		if (table_rate >= rate) {
			if (!up && i && (table_rate > rate)) {
				i--;
				table_rate = tegra_emc_table[i].rate;
			}
			pr_debug("%s: using %lu\n", __func__, table_rate);
			last_round_idx = i;
			return table_rate * 1000;
		}
	}

	return -EINVAL;
}

struct clk *tegra_emc_predict_parent(unsigned long rate, u32 *div_value)
{
	int i;
	unsigned long pll_rate;
	struct clk *p, *p_new;

	if (!tegra_emc_table) {
		if (rate == clk_get_rate_locked(emc)) {
			*div_value = emc->div - 2;
			return emc->parent;
		}
		return NULL;
	}

	pr_debug("%s: %lu\n", __func__, rate);

	/* Table entries specify rate in kHz */
	rate = rate / 1000;

	i = get_start_idx(rate);
	for (; i < tegra_emc_table_size; i++) {
		if (tegra_emc_table[i].rate == rate) {
			p_new = tegra_emc_clk_sel[i].input;
			if (!p_new)
				continue;

			pll_rate = tegra_emc_clk_sel[i].input_rate;
			*div_value = (tegra_emc_clk_sel[i].value &
				      EMC_CLK_EMC_2X_CLK_DIVISOR_MASK) >>
				EMC_CLK_EMC_2X_CLK_DIVISOR_SHIFT;

			/*
			 * pll_m/pll_mb ping-pong:
			 * - select current parent when its rate matches table
			 * - select pll_m or pll_mb, when it is not current
			 *   parent; set pll rate if it is not matching table
			 */
			p = clk_get_parent(emc);
			if (pll_rate == clk_get_rate(p))
				return p;

			if (p_new != p) {
				int ret = 0;
				if (pll_rate != clk_get_rate(p_new))
					ret = clk_set_rate(p_new, pll_rate);
				if (!ret)
					return p_new;
			}

			p_new = tegra_emc_clk_sel_b[i].input;
			if (p_new != p) {
				if (pll_rate != clk_get_rate(p_new)) {
					if (clk_set_rate(p_new, pll_rate))
						return NULL;
				}
				return p_new;
			}
		}
	}
	return NULL;
}

static inline const struct clk_mux_sel *get_emc_input(u32 val)
{
	const struct clk_mux_sel *sel;

	for (sel = emc->inputs; sel->input != NULL; sel++) {
		if (sel->value == val)
			break;
	}
	return sel;
}

static int find_matching_input(const struct tegra21_emc_table *table,
	struct clk *pll_m, struct clk *pll_mb, int sel_idx)
{
	u32 div_value = (table->src_sel_reg &
			 EMC_CLK_EMC_2X_CLK_DIVISOR_MASK) >>
		EMC_CLK_EMC_2X_CLK_DIVISOR_SHIFT;
	u32 src_value = (table->src_sel_reg & EMC_CLK_EMC_2X_CLK_SRC_MASK) >>
		EMC_CLK_EMC_2X_CLK_SRC_SHIFT;

	unsigned long input_rate = 0;
	unsigned long table_rate = table->rate * 1000; /* table rate in kHz */
	struct emc_sel *emc_clk_sel = &tegra_emc_clk_sel[sel_idx];
	struct emc_sel *emc_clk_sel_b = &tegra_emc_clk_sel_b[sel_idx];
	const struct clk_mux_sel *sel = get_emc_input(src_value);

	if (div_value & 0x1) {
		pr_warn("tegra: invalid odd divider for EMC rate %lu\n",
			table_rate);
		return -EINVAL;
	}
	if (!sel->input) {
		pr_warn("tegra: no matching input found for EMC rate %lu\n",
			table_rate);
		return -EINVAL;
	}

	if (!(table->src_sel_reg & EMC_CLK_MC_EMC_SAME_FREQ) !=
	    !(MC_EMEM_ARB_MISC0_EMC_SAME_FREQ &
	      table->burst_mc_regs[MC_EMEM_ARB_MISC0_INDEX])) {
		pr_warn("tegra: ambiguous EMC to MC ratio for EMC rate %lu\n",
			table_rate);
		return -EINVAL;
	}

	if (sel->input == pll_m) {
		/* pll_m(b) can scale to match target rate */
		input_rate = table_rate * (1 + div_value / 2);
	} else {
		/* all other sources are fixed, must exactly match the rate */
		input_rate = clk_get_rate(sel->input);
		if (input_rate != (table_rate * (1 + div_value / 2))) {
			pr_warn("tegra: EMC rate %lu does not match %s rate %lu\n",
				table_rate, sel->input->name, input_rate);
			return -EINVAL;
		}
	}

	/* Get ready emc clock selection settings for this table rate */
	emc_clk_sel->input = sel->input;
	emc_clk_sel->input_rate = input_rate;
	emc_clk_sel->value = table->src_sel_reg;

	emc_clk_sel_b->input = sel->input;
	emc_clk_sel_b->input_rate = input_rate;
	emc_clk_sel_b->value = table->src_sel_reg;

	/* Replace PLLM with PLLMB is PLLMB selection able */
	if (pll_mb && (sel->input == pll_m)) {
		u32 src_value_b = src_value == EMC_CLK_SOURCE_PLLM_LJ ?
			EMC_CLK_SOURCE_PLLMB_LJ : EMC_CLK_SOURCE_PLLMB;
		emc_clk_sel_b->input = pll_mb;
		emc_clk_sel_b->value &= ~EMC_CLK_EMC_2X_CLK_SRC_MASK;
		emc_clk_sel_b->value |= src_value_b <<
			EMC_CLK_EMC_2X_CLK_SRC_SHIFT;
	}

	return 0;
}


static int emc_core_millivolts[MAX_DVFS_FREQS];

static void adjust_emc_dvfs_table(const struct tegra21_emc_table *table,
				  int table_size)
{
	int i, j, mv;
	unsigned long rate;

	BUG_ON(table_size > MAX_DVFS_FREQS);

	for (i = 0, j = 0; j < table_size; j++) {
		if (tegra_emc_clk_sel[j].input == NULL)
			continue;	/* invalid entry */

		rate = table[j].rate * 1000;
		mv = table[j].emc_min_mv;

		if ((i == 0) || (mv > emc_core_millivolts[i-1])) {
			/* advance: voltage has increased */
			emc->dvfs->freqs[i] = rate;
			emc_core_millivolts[i] = mv;
			i++;
		} else {
			/* squash: voltage has not increased */
			emc->dvfs->freqs[i-1] = rate;
		}
	}

	emc->dvfs->millivolts = emc_core_millivolts;
	emc->dvfs->num_freqs = i;
}

/*
 * pll_m can be scaled provided pll_mb is available;
 * if not - remove rates that require pll_m scaling
 */
static int purge_emc_table(unsigned long max_rate)
{
	int i;
	int ret = 0;

	pr_warn("tegra: cannot scale pll_m since pll_mb is not available:\n");
	pr_warn("       removed not supported entries from the table:\n");

	/* made all entries with non matching rate invalid */
	for (i = 0; i < tegra_emc_table_size; i++) {
		struct emc_sel *sel = &tegra_emc_clk_sel[i];
		struct emc_sel *sel_b = &tegra_emc_clk_sel_b[i];
		if (sel->input) {
			if (clk_get_rate(sel->input) != sel->input_rate) {
				pr_warn("       EMC rate %lu\n",
					tegra_emc_table[i].rate * 1000);
				sel->input = NULL;
				sel->input_rate = 0;
				sel->value = 0;
				*sel_b = *sel;
				if (max_rate == tegra_emc_table[i].rate)
					ret = -EINVAL;
			}
		}
	}
	return ret;
}

static int init_emc_table(const struct tegra21_emc_table *table,
			  const struct tegra21_emc_table *table_der,
			  int table_size)
{
	int i, mv;
	bool max_entry = false;
	bool emc_max_dvfs_sel = 1; /* FIXME: restore get_emc_max_dvfs(); */
	unsigned long boot_rate, max_rate;
	struct clk *pll_m = tegra_get_clock_by_name("pll_m");
	struct clk *pll_mb = tegra_get_clock_by_name("pll_mb");

	if (!tegra_clk_is_parent_allowed(emc, pll_mb)) {
		WARN(1, "tegra: PLLMB can not be used for EMC DVFS\n");
		pll_mb = NULL;
	}

	emc_stats.clkchange_count = 0;
	spin_lock_init(&emc_stats.spinlock);
	emc_stats.last_update = get_jiffies_64();
	emc_stats.last_sel = TEGRA_EMC_TABLE_MAX_SIZE;

	if ((dram_type != DRAM_TYPE_LPDDR4) &&
	    (dram_type != DRAM_TYPE_LPDDR2) &&
	    (dram_type != DRAM_TYPE_DDR3)) {
		pr_err("tegra: not supported DRAM type %u\n", dram_type);
		return -ENODATA;
	}

	if (!table || !table_size) {
		pr_err("tegra: EMC DFS table is empty\n");
		return -ENODATA;
	}

	boot_rate = clk_get_rate(emc) / 1000;
	max_rate = boot_rate;

	tegra_emc_table_size = min(table_size, TEGRA_EMC_TABLE_MAX_SIZE);
	switch (table[0].rev) {
	case 0x5:
		start_timing.burst_regs_num = table[0].burst_regs_num;
		break;
	default:
		pr_err("tegra: invalid EMC DFS table: unknown rev 0x%x\n",
			table[0].rev);
		return -ENODATA;
	}

	if (table_der) {
		/* Check that the derated table and non-derated table match. */
		for (i = 0; i < tegra_emc_table_size; i++) {
			if (table[i].rate        != table_der[i].rate ||
			    table[i].rev         != table_der[i].rev ||
			    table[i].emc_min_mv  != table_der[i].emc_min_mv ||
			    table[i].src_sel_reg != table_der[i].src_sel_reg) {
				pr_err("tegra: emc: Derated table mismatch.\n");
				return -EINVAL;
			}
		}
		pr_info("tegra: emc: Derated table is valid.\n");
	}

	/* Match EMC source/divider settings with table entries */
	for (i = 0; i < tegra_emc_table_size; i++) {
		unsigned long table_rate = table[i].rate;

		/* Stop: "no-rate" entry, or entry violating ascending order */
		if (!table_rate || (i && ((table_rate <= table[i-1].rate) ||
			(table[i].emc_min_mv < table[i-1].emc_min_mv)))) {
			pr_warn("tegra: EMC rate entry %lu is not ascending\n",
				table_rate);
			break;
		}

		BUG_ON(table[i].rev != table[0].rev);

		if (find_matching_input(&table[i], pll_m, pll_mb, i))
			continue;

		if (table_rate == boot_rate)
			emc_stats.last_sel = i;

		if (emc_max_dvfs_sel) {
			/* EMC max rate = max table entry above boot rate */
			if (table_rate >= max_rate) {
				max_rate = table_rate;
				max_entry = true;
			}
		} else if (table_rate == max_rate) {
			/* EMC max rate = boot rate */
			max_entry = true;
			break;
		}
	}

	/* Validate EMC rate and voltage limits */
	if (!max_entry) {
		pr_err("tegra: invalid EMC DFS table: entry for max rate"
		       " %lu kHz is not found\n", max_rate);
		return -ENODATA;
	}

	if (emc_stats.last_sel == TEGRA_EMC_TABLE_MAX_SIZE) {
		pr_err("tegra: invalid EMC DFS table: entry for boot rate"
		       " %lu kHz is not found\n", boot_rate);
		return -ENODATA;
	}

	tegra_emc_table = table;
	tegra_emc_table_derated = table_der;

	/*
	 * Purge rates that cannot be reached because PLLMB can not be used
	 * If maximum rate was purged, do not install table.
	 */
	if (!pll_mb && purge_emc_table(max_rate)) {
		pr_err("tegra: invalid EMC DFS table: entry for max rate"
		       " %lu kHz can not be reached\n", max_rate);
		return -ENODATA;
	}
	tegra_init_max_rate(emc, max_rate * 1000);

	if (emc->dvfs) {
		adjust_emc_dvfs_table(tegra_emc_table, tegra_emc_table_size);
		mv = tegra_dvfs_predict_mv_at_hz_max_tfloor(emc, max_rate*1000);
		if ((mv <= 0) || (mv > emc->dvfs->max_millivolts)) {
			tegra_emc_table = NULL;
			pr_err("tegra: invalid EMC DFS table: maximum rate %lu"
			       " kHz does not match nominal voltage %d\n",
			       max_rate, emc->dvfs->max_millivolts);
			return -ENODATA;
		}
	}

	pr_info("tegra: validated EMC DFS table\n");

	return 0;
}

#ifdef CONFIG_PASR
static bool tegra21_is_lpddr3(void)
{
	return (dram_type == DRAM_TYPE_LPDDR2);
}

static void tegra21_pasr_apply_mask(u16 *mem_reg, void *cookie)
{
	u32 val = 0;
	int device = (int)(uintptr_t)cookie;

	val = TEGRA_EMC_MODE_REG_17 | *mem_reg;
	val |= device << TEGRA_EMC_MRW_DEV_SHIFT;

	emc_writel(val, EMC_MRW);

	pr_debug("%s: cookie = %d mem_reg = 0x%04x val = 0x%08x\n", __func__,
			(int)(uintptr_t)cookie, *mem_reg, val);
}

static void tegra21_pasr_remove_mask(phys_addr_t base, void *cookie)
{
	u16 mem_reg = 0;

	if (!pasr_register_mask_function(base, NULL, cookie))
			tegra21_pasr_apply_mask(&mem_reg, cookie);

}

static int tegra21_pasr_set_mask(phys_addr_t base, void *cookie)
{
	return pasr_register_mask_function(base, &tegra21_pasr_apply_mask,
					cookie);
}

static int tegra21_pasr_enable(const char *arg, const struct kernel_param *kp)
{
	unsigned int old_pasr_enable;
	void *cookie;
	int num_devices;
	u64 device_size;
	u64 size_mul;
	int ret = 0;

	if (!tegra21_is_lpddr3())
		return -ENOSYS;

	old_pasr_enable = pasr_enable;
	param_set_int(arg, kp);

	if (old_pasr_enable == pasr_enable)
		return ret;

	num_devices = 1 << (mc_readl(MC_EMEM_ADR_CFG) & BIT(0));
	size_mul = 1 << ((emc_readl(EMC_FBIO_CFG5) >> 4) & BIT(0));

	/* Cookie represents the device number to write to MRW register.
	 * 0x2 to for only dev0, 0x1 for dev1.
	 */
	if (pasr_enable == 0) {
		cookie = (void *)(int)TEGRA_EMC_MRW_DEV1;

		tegra21_pasr_remove_mask(TEGRA_DRAM_BASE, cookie);

		if (num_devices == 1)
			goto exit;

		cookie = (void *)(int)TEGRA_EMC_MRW_DEV2;
		/* Next device is located after first device, so read DEV0 size
		 * to decide base address for DEV1 */
		device_size = 1 << ((mc_readl(MC_EMEM_ADR_CFG_DEV0) >>
					MC_EMEM_DEV_SIZE_SHIFT) &
					MC_EMEM_DEV_SIZE_MASK);
		device_size = device_size * size_mul * SZ_4M;

		tegra21_pasr_remove_mask(TEGRA_DRAM_BASE + device_size, cookie);
	} else {
		cookie = (void *)(int)TEGRA_EMC_MRW_DEV1;

		ret = tegra21_pasr_set_mask(TEGRA_DRAM_BASE, cookie);

		if (num_devices == 1 || ret)
			goto exit;

		cookie = (void *)(int)TEGRA_EMC_MRW_DEV2;

		/* Next device is located after first device, so read DEV0 size
		 * to decide base address for DEV1 */
		device_size = 1 << ((mc_readl(MC_EMEM_ADR_CFG_DEV0) >>
					MC_EMEM_DEV_SIZE_SHIFT) &
					MC_EMEM_DEV_SIZE_MASK);
		device_size = device_size * size_mul * SZ_4M;

		ret = tegra21_pasr_set_mask(TEGRA_DRAM_BASE + device_size, cookie);
	}

exit:
	return ret;
}

static struct kernel_param_ops tegra21_pasr_enable_ops = {
	.set = tegra21_pasr_enable,
	.get = param_get_int,
};
module_param_cb(pasr_enable, &tegra21_pasr_enable_ops, &pasr_enable, 0644);
#endif

/* FIXME: add to clock resume */
void tegra21_mc_holdoff_enable(void)
{
	mc_writel(HYST_DISPLAYHCB | HYST_DISPLAYHC |
		HYST_DISPLAY0CB | HYST_DISPLAY0C | HYST_DISPLAY0BB |
		HYST_DISPLAY0B | HYST_DISPLAY0AB | HYST_DISPLAY0A,
		MC_EMEM_ARB_HYSTERESIS_0_0);
	mc_writel(HYST_VDEDBGW | HYST_VDEBSEVW | HYST_NVENCSWR,
		MC_EMEM_ARB_HYSTERESIS_1_0);
	mc_writel(HYST_DISPLAYT | HYST_GPUSWR | HYST_ISPWBB |
		HYST_ISPWAB | HYST_ISPWB | HYST_ISPWA |
		HYST_VDETPMW | HYST_VDEMBEW,
		MC_EMEM_ARB_HYSTERESIS_2_0);
	mc_writel(HYST_DISPLAYD | HYST_VIW | HYST_VICSWR,
		MC_EMEM_ARB_HYSTERESIS_3_0);
}

static int tegra21_emc_probe(struct platform_device *pdev)
{
	struct tegra21_emc_pdata *pdata;
	struct resource *res;
	int ret;

	if (tegra_emc_table) {
		ret = -EINVAL;
		goto out;
	}

	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
	if (!res) {
		dev_err(&pdev->dev, "missing register base\n");
		ret = -ENOMEM;
		goto out;
	}

	pdata = tegra_emc_dt_parse_pdata(pdev);

	if (!pdata) {
		dev_err(&pdev->dev, "missing platform data\n");
		ret = -ENODATA;
		goto out;
	}

	pr_info("Loading EMC tables...\n");
	ret = init_emc_table(pdata->tables, pdata->tables_derated,
			      pdata->num_tables);

	if (!ret) {
		tegra_emc_iso_usage_table_init(tegra21_emc_iso_usage,
				ARRAY_SIZE(tegra21_emc_iso_usage));
		if (emc_enable) {
			unsigned long rate = tegra_emc_round_rate_updown(
				emc->boot_rate, false);
			if (!IS_ERR_VALUE(rate))
				tegra_clk_preset_emc_monitor(rate);
		}
	}

out:
	return ret;
}

static struct of_device_id tegra21_emc_of_match[] = {
	{ .compatible = "nvidia,tegra21-emc", },
	{ },
};

static struct platform_driver tegra21_emc_driver = {
	.driver         = {
		.name   = "tegra-emc",
		.owner  = THIS_MODULE,
		.of_match_table = tegra21_emc_of_match
	},
	.probe          = tegra21_emc_probe,
};

int __init tegra21_emc_init(void)
{
	return platform_driver_register(&tegra21_emc_driver);
}

void tegra_emc_timing_invalidate(void)
{
	emc_timing = NULL;
	tegra_mc_divider_update(emc);
}

void tegra_emc_dram_type_init(struct clk *c)
{
	emc = c;

	dram_type = (emc_readl(EMC_FBIO_CFG5) &
		     EMC_FBIO_CFG5_DRAM_TYPE_MASK) >>
		EMC_FBIO_CFG5_DRAM_TYPE_SHIFT;

	dram_dev_num = (mc_readl(MC_EMEM_ADR_CFG) & 0x1) + 1; /* 2 dev max */
}

int tegra_emc_get_dram_type(void)
{
	return dram_type;
}

static int emc_read_mrr(int dev, int addr)
{
	int ret;
	u32 val, emc_cfg;

	if (dram_type != DRAM_TYPE_LPDDR2)
		return -ENODEV;

	ret = wait_for_update(EMC_EMC_STATUS,
			      EMC_EMC_STATUS_MRR_DIVLD, false, 0);
	if (ret)
		return ret;

	emc_cfg = emc_readl(EMC_CFG);
	if (emc_cfg & EMC_CFG_DRAM_ACPD) {
		emc_writel(emc_cfg & ~EMC_CFG_DRAM_ACPD, EMC_CFG);
		emc_timing_update(0);
	}

	val = dev ? DRAM_DEV_SEL_1 : DRAM_DEV_SEL_0;
	val |= (addr << EMC_MRR_MA_SHIFT) & EMC_MRR_MA_MASK;
	emc_writel(val, EMC_MRR);

	ret = wait_for_update(EMC_EMC_STATUS,
			      EMC_EMC_STATUS_MRR_DIVLD, true, 0);
	if (emc_cfg & EMC_CFG_DRAM_ACPD) {
		emc_writel(emc_cfg, EMC_CFG);
		emc_timing_update(0);
	}
	if (ret)
		return ret;

	val = emc_readl(EMC_MRR) & EMC_MRR_DATA_MASK;
	return val;
}

int tegra_emc_get_dram_temperature(void)
{
	int mr4 = 0;
	unsigned long flags;

	spin_lock_irqsave(&emc_access_lock, flags);

	mr4 = emc_read_mrr(0, 4);
	if (IS_ERR_VALUE(mr4)) {
		spin_unlock_irqrestore(&emc_access_lock, flags);
		return mr4;
	}

	spin_unlock_irqrestore(&emc_access_lock, flags);

	mr4 = (mr4 & LPDDR2_MR4_TEMP_MASK) >> LPDDR2_MR4_TEMP_SHIFT;
	return mr4;
}

int tegra_emc_set_over_temp_state(unsigned long state)
{
	int offset;
	unsigned long flags;
	const struct tegra21_emc_table *current_table;
	const struct tegra21_emc_table *new_table;

	if (dram_type != DRAM_TYPE_LPDDR2 || !emc_timing)
		return -ENODEV;

	if (state > DRAM_OVER_TEMP_THROTTLE)
		return -EINVAL;

	/* Silently do nothing if there is no state change. */
	if (state == dram_over_temp_state)
		return 0;

	/*
	 * If derating needs to be turned on/off force a clock change. That
	 * will take care of the refresh as well. In derating is not going to
	 * be changed then all that is needed is an update to the refresh
	 * settings.
	 */
	spin_lock_irqsave(&emc_access_lock, flags);

	current_table = emc_get_table(dram_over_temp_state);
	new_table = emc_get_table(state);
	dram_over_temp_state = state;

	if (current_table != new_table) {
		offset = emc_timing - current_table;
		emc_set_clock(&new_table[offset], emc_timing, 0,
			new_table[offset].src_sel_reg |
			EMC_CLK_FORCE_CC_TRIGGER);
		emc_timing = &new_table[offset];
		tegra_mc_divider_update(emc);
	} else {
		set_over_temp_timing(emc_timing, state);
		emc_timing_update(0);
		if (state != DRAM_OVER_TEMP_NONE)
			emc_writel(EMC_REF_FORCE_CMD, EMC_REF);
	}

	spin_unlock_irqrestore(&emc_access_lock, flags);

	pr_debug("[emc] %s: temp_state: %lu  - selected %s table\n",
		__func__, dram_over_temp_state,
		new_table == tegra_emc_table ? "regular" : "derated");

	return 0;
}


#ifdef CONFIG_TEGRA_USE_NCT
int tegra21_nct_emc_table_init(struct tegra21_emc_pdata *nct_emc_pdata)
{
	union nct_item_type *entry = NULL;
	struct tegra21_emc_table *mem_table_ptr;
	u8 *src, *dest;
	unsigned int i, non_zero_freqs;
	int ret = 0;

	/* Allocating memory for holding a single NCT entry */
	entry = kmalloc(sizeof(union nct_item_type), GFP_KERNEL);
	if (!entry) {
		pr_err("%s: failed to allocate buffer for single entry. ",
								__func__);
		ret = -ENOMEM;
		goto done;
	}
	src = (u8 *)entry;

	/* Counting the actual number of frequencies present in the table */
	non_zero_freqs = 0;
	for (i = 0; i < TEGRA_EMC_MAX_FREQS; i++) {
		if (!tegra_nct_read_item(NCT_ID_MEMTABLE + i, entry)) {
			if (entry->tegra_emc_table.tegra21_emc_table.rate > 0) {
				non_zero_freqs++;
				pr_info("%s: Found NCT item for freq %lu.\n",
				 __func__,
				 entry->tegra_emc_table.tegra21_emc_table.rate);
			} else
				break;
		} else {
			pr_err("%s: NCT: Could not read item for %dth freq.\n",
								__func__, i);
			ret = -EIO;
			goto free_entry;
		}
	}

	/* Allocating memory for the DVFS table */
	mem_table_ptr = kmalloc(sizeof(struct tegra21_emc_table) *
				non_zero_freqs, GFP_KERNEL);
	if (!mem_table_ptr) {
		pr_err("%s: Memory allocation for emc table failed.",
							    __func__);
		ret = -ENOMEM;
		goto free_entry;
	}

	/* Copy paste the emc table from NCT partition */
	for (i = 0; i < non_zero_freqs; i++) {
		/*
		 * We reset the whole buffer, to emulate the property
		 * of a static variable being initialized to zero
		 */
		memset(entry, 0, sizeof(*entry));
		ret = tegra_nct_read_item(NCT_ID_MEMTABLE + i, entry);
		if (!ret) {
			dest = (u8 *)mem_table_ptr + (i * sizeof(struct
							tegra21_emc_table));
			memcpy(dest, src, sizeof(struct tegra21_emc_table));
		} else {
			pr_err("%s: Could not copy item for %dth freq.\n",
								__func__, i);
			goto free_mem_table_ptr;
		}
	}

	/* Setting appropriate pointers */
	nct_emc_pdata->tables = mem_table_ptr;
	nct_emc_pdata->num_tables = non_zero_freqs;

	goto free_entry;

free_mem_table_ptr:
	kfree(mem_table_ptr);
free_entry:
	kfree(entry);
done:
	return ret;
}
#endif

/*
 * Given the passed ISO BW find the index into the table of ISO efficiencies.
 */
static inline int get_iso_bw_table_idx(unsigned long iso_bw)
{
	int i = ARRAY_SIZE(iso_bw_table) - 1;

	while (i > 0 && iso_bw_table[i] > iso_bw)
		i--;

	return i;
}

/*
 * Return the ISO BW efficiency for the attached DRAM type at the passed ISO BW.
 * This is used for when only the display is active - OS IDLE.
 *
 * For now when the DRAM is being temperature throttled return the normal ISO
 * efficiency. This will have to change once the throttling efficiency data
 * becomes available.
 */
static u8 get_iso_bw_os_idle(unsigned long iso_bw)
{
	int freq_idx = get_iso_bw_table_idx(iso_bw);

	/* On T21- LPDDR2 means LPDDR3. */
	if (dram_type == DRAM_TYPE_LPDDR2) {
		if (dram_over_temp_state == DRAM_OVER_TEMP_THROTTLE)
			return tegra21_lpddr3_iso_efficiency_os_idle[freq_idx];
		else
			return tegra21_lpddr3_iso_efficiency_os_idle[freq_idx];
	} else if (dram_type == DRAM_TYPE_DDR3) {
		if (dram_over_temp_state == DRAM_OVER_TEMP_THROTTLE)
			return tegra21_ddr3_iso_efficiency_os_idle[freq_idx];
		else
			return tegra21_ddr3_iso_efficiency_os_idle[freq_idx];
	} else { /* LPDDR4 */
		if (dram_over_temp_state == DRAM_OVER_TEMP_THROTTLE)
			return tegra21_lpddr4_iso_efficiency_os_idle[freq_idx];
		else
			return tegra21_lpddr4_iso_efficiency_os_idle[freq_idx];
	}
}

/*
 * Same as get_iso_bw_os_idle() only this is used for when there are other
 * engines aside from display running.
 */
static u8 get_iso_bw_general(unsigned long iso_bw)
{
	int freq_idx = get_iso_bw_table_idx(iso_bw);

	/* On T21- LPDDR2 means LPDDR3. */
	if (dram_type == DRAM_TYPE_LPDDR2) {
		if (dram_over_temp_state == DRAM_OVER_TEMP_THROTTLE)
			return tegra21_lpddr3_iso_efficiency_general[freq_idx];
		else
			return tegra21_lpddr3_iso_efficiency_general[freq_idx];
	} else if (dram_type == DRAM_TYPE_DDR3) {
		if (dram_over_temp_state == DRAM_OVER_TEMP_THROTTLE)
			return tegra21_ddr3_iso_efficiency_general[freq_idx];
		else
			return tegra21_ddr3_iso_efficiency_general[freq_idx];
	} else { /* LPDDR4 */
		if (dram_over_temp_state == DRAM_OVER_TEMP_THROTTLE)
			return tegra21_lpddr4_iso_efficiency_general[freq_idx];
		else
			return tegra21_lpddr4_iso_efficiency_general[freq_idx];
	}
}

#ifdef CONFIG_DEBUG_FS

static struct dentry *emc_debugfs_root;

static int emc_stats_show(struct seq_file *s, void *data)
{
	int i;

	emc_last_stats_update(TEGRA_EMC_TABLE_MAX_SIZE);

	seq_printf(s, "%-10s %-10s\n", "rate kHz", "time");
	for (i = 0; i < tegra_emc_table_size; i++) {
		if (tegra_emc_clk_sel[i].input == NULL)
			continue;	/* invalid entry */

		seq_printf(s, "%-10lu %-10llu\n", tegra_emc_table[i].rate,
			cputime64_to_clock_t(emc_stats.time_at_clock[i]));
	}
	seq_printf(s, "%-15s %llu\n", "transitions:",
		   emc_stats.clkchange_count);
	seq_printf(s, "%-15s %llu\n", "time-stamp:",
		   cputime64_to_clock_t(emc_stats.last_update));

	return 0;
}

static int emc_stats_open(struct inode *inode, struct file *file)
{
	return single_open(file, emc_stats_show, inode->i_private);
}

static const struct file_operations emc_stats_fops = {
	.open		= emc_stats_open,
	.read		= seq_read,
	.llseek		= seq_lseek,
	.release	= single_release,
};

static int emc_table_info_show(struct seq_file *s, void *data)
{
	int i;
	for (i = 0; i < tegra_emc_table_size; i++) {
		if (tegra_emc_clk_sel[i].input == NULL)
			continue;
		seq_printf(s, "Table info:\n   Rev: 0x%02x\n"
		"   Table ID: %s\n", tegra_emc_table[i].rev,
		tegra_emc_table[i].table_id);
		seq_printf(s, "    %lu\n", tegra_emc_table[i].rate);
	}

	return 0;
}

static int emc_table_info_open(struct inode *inode, struct file *file)
{
	return single_open(file, emc_table_info_show, inode->i_private);
}

static const struct file_operations emc_table_info_fops = {
	.open		= emc_table_info_open,
	.read		= seq_read,
	.llseek		= seq_lseek,
	.release	= single_release,
};

static int dram_temperature_get(void *data, u64 *val)
{
	*val = tegra_emc_get_dram_temperature();
	return 0;
}
DEFINE_SIMPLE_ATTRIBUTE(dram_temperature_fops, dram_temperature_get,
			NULL, "%lld\n");

static int over_temp_state_get(void *data, u64 *val)
{
	*val = dram_over_temp_state;
	return 0;
}
static int over_temp_state_set(void *data, u64 val)
{
	return tegra_emc_set_over_temp_state(val);
}
DEFINE_SIMPLE_ATTRIBUTE(over_temp_state_fops, over_temp_state_get,
			over_temp_state_set, "%llu\n");

static int efficiency_get(void *data, u64 *val)
{
	*val = tegra_emc_bw_efficiency;
	return 0;
}
static int efficiency_set(void *data, u64 val)
{
	tegra_emc_bw_efficiency = (val > 100) ? 100 : val;
	if (emc)
		tegra_clk_shared_bus_update(emc);

	return 0;
}
DEFINE_SIMPLE_ATTRIBUTE(efficiency_fops, efficiency_get,
			efficiency_set, "%llu\n");

static int __init tegra_emc_debug_init(void)
{
	if (!tegra_emc_table)
		return 0;

	emc_debugfs_root = debugfs_create_dir("tegra_emc", NULL);
	if (!emc_debugfs_root)
		return -ENOMEM;

	if (!debugfs_create_file(
		"stats", S_IRUGO, emc_debugfs_root, NULL, &emc_stats_fops))
		goto err_out;

	if (!debugfs_create_u32("clkchange_delay", S_IRUGO | S_IWUSR,
		emc_debugfs_root, (u32 *)&clkchange_delay))
		goto err_out;

	/*
	 * Reading dram temperature supported only for LP DDR variants,
	 * Currently two variants of DDR are supported i.e. LPDDR2 and DDR3
	 */
	if (dram_type == DRAM_TYPE_LPDDR2 &&
		!debugfs_create_file("dram_temperature",
		S_IRUGO, emc_debugfs_root, NULL, &dram_temperature_fops))
		goto err_out;

	if (!debugfs_create_file("over_temp_state", S_IRUGO | S_IWUSR,
				emc_debugfs_root, NULL, &over_temp_state_fops))
		goto err_out;

	if (!debugfs_create_file("efficiency", S_IRUGO | S_IWUSR,
				 emc_debugfs_root, NULL, &efficiency_fops))
		goto err_out;


	if (tegra_emc_iso_usage_debugfs_init(emc_debugfs_root))
		goto err_out;

	if (!debugfs_create_file("table_info", S_IRUGO,
				 emc_debugfs_root, NULL, &emc_table_info_fops))
		goto err_out;

	return 0;

err_out:
	debugfs_remove_recursive(emc_debugfs_root);
	return -ENOMEM;
}

late_initcall(tegra_emc_debug_init);
#endif
